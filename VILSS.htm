<html>
<head>
<title>University of Bristol - Visual Information Laboratory Seminar Series (VILSS)</title>
<link rel="stylesheet" href="simple.css"> 
</head>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-7551853-2");
pageTracker._trackPageview();
} catch(err) {}</script>
<head>
<body>

    <h1>Visual Information Laboratory Seminar Series (VILSS)</h1>

<table border="0" cellpadding="5" cellspacing="0" width="100%">

<h3> Upcoming Seminars </h3>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><b>YEAR</b></td>
<td><b>DATE</b></td>
<td><b>SPEAKER</b></td>
    <td><b>TITLE</b></td></tr>
    
    <tr><td align="right" valign="top" bgcolor="#f0f0f0">
2015
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Nov 17
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Mengyang Yu<br/><i>Northumbria University</i><br/><img height="100" alt="Mengyang Yu" border="1" src="https://media.licdn.com/mpr/mpr/shrinknp_400_400/p/3/000/2cf/1f2/0f24963.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0">
        <b>Discriminative Feature Learning for Large-scale Data</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>TBC</font>
</td></tr>
        <tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Dec 8
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Aryana Tavanai<br/><i>University of Leeds</i><br/><img height="100" alt="Mengyang Yu" border="1" src="http://www.comp.leeds.ac.uk/fy06at/Files/Images/Aryana_Tavanai.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0">
        <b>Joint Tracking and Event Analysis for Carried Object Detection</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>
           Tracking and Event Analysis are areas of video analysis which have great importance in robotics applications and automated surveillance. Although they have been greatly studied individually, there has been little work on performing them jointly where they mutually influence and improve each other. In this talk I will present our novel approach for jointly estimating the track of a moving object and recognising the events in which it participates. <br/>

First, I will introduce our geometric carried object detector. Then I will present our tracklet building approach which enforces spatial consistency between the carried objects and other pre-tracked entities in the scene. Finally, I will present our joint tracking and event analysis framework posed as maximisation of a posterior probability defined over event sequences and temporally-disjoint subsets of tracklets. We evaluate our approach using tracklets from three state of the art trackers and demonstrate improved tracking performance in each case, as a result of jointly incorporating events, while also subsequently improving event recognition.</font>
</td></tr>



</table>
<hr/>
<h3> Past Seminars </h3>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><b>YEAR</b></td>
<td><b>DATE</b></td>
<td><b>SPEAKER</b></td>
<td><b>TITLE</b></td>
</tr>
    <tr><td align="right" valign="top" bgcolor="#f0f0f0">
2015
</td><td nowrap valign="top" bgcolor="#f0f0f0">
June 16
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Denis Kouame<br/><i> Universite Paul Sabatier</i><br/> <i>Toulouse</i><br/><img height="100" alt="Denis Kouame" width="75" border="1" src="./VILSS:Material/Denis.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Ultrasound imaging and inverse problems</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>Among all the medical imaging modalities, ultrasound imaging is the most widely used, due to its safety, cost-effectiveness, flexibility and real-time nature. However, compared to other medical imaging modalities such as Magnetic Resonance Imaging (MRI), or Computed Tomography (CT), ultrasound images suffers from the presence of speckle and have low-resolution in most standard applications. Although most manufacturers of ultrasound scanners have developed many device-based-routines in order to overcome these issues, many challenges in terms of signal and image processing remain. In this tutorial, we will review the basics and advanced ultrasound imaging, then we will focus on the current signal and image processing challenges, and show some recent results.</font> 
</td></tr>
<tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
May 12
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Adrian Bors<br/><i>Department of Computer Science</i><br/><i>University of York</i><br/><img height="100" alt="Adrian Bors" width="75" border="1" src="http://www-users.cs.york.ac.uk/adrian/AdrianBors.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Ortho-diffusion decompositions of graph-based representations of images</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>In this presentation I introduce the ortho-diffusion operator.
I consider graph-based data representations where full data
interconnectivity is modelled using probability transition matrices.
Multi-scale dimensionality reduction at different scales is used
in order to extract the meaningful data representations.
The QR orthonormal decomposition algorithm, alternating
with diffusion and data reduction stages is applied recursively
at each scale level for the given data representation.
Columns in the ortho-diffusion representation matrix represent 
characteristic features of the data. Those columns that are not 
considered essential for the data representation are removed at each
scale. The proposed methodology is used to model features extracted 
from images which are then used for image matching and face 
recognition. Image matching is applied to optical flow estimation
from image sequences. For the face recognition application I consider 
both global appearance models, based on either the correlation or 
the covariance of training sets, as well as semantic representations 
of biometric features. The proposed methodology is shown to be robust 
in face classification applications when considering image corruption 
by various noise statistics.</font> 
</td></tr>
    <tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Apr 21
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Teo de Campos<br/><i>Centre for Vision, <br/>Speech and Signal Processing</i><br/><i>University of Surrey</i><br/><img height="100" alt="Teo" width="75" border="1" src="http://www.surrey.ac.uk/cvssp/images/people/teo_de_campos_thumbnail.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0"> 
<b>Transductive Transfer Learning for Computer Vision</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>One of the ultimate goals of the open ended learning systems is to take advantage of previous experience in dealing with future problems. We focus on classification problems where labelled samples are available in a known problem (the source domain), but when the system is deployed in the target dataset, the distribution of samples is different. Although the number of classes and the feature extraction method remain the same, a change of domain happens because there is a difference between the typical distribution of data of source and target samples. This is a very common situation in computer vision applications, e.g., when a synthetic dataset is used for training but the system is applied on images "in the wild".
We assume that a set of unlabelled samples is available target domain. This constitutes a Transductive Transfer Learning problem, also known as Unsupervised Domain Adaptation.

<p></p>We proposed to tackle this problem by adapting the feature space of the source domain samples, so that their distribution becomes more similar to that of the target domain samples. Therefore a classifier re-trained on the updated source space can give better results on the target samples.
We proposed to use a pipeline which consists of three main components: <br/>
(i) a method for global adaptation of the marginal distribution of the data using Maximum Mean Discrepancy;<br/>
(ii) a sample-based adaptation method, which translates each source sample towards the distribution of the target samples;<br/>
(iii) a class-based conditional distribution adaptation method.<br/>
We conducted experiments on a range of image classification and action recognition datasets and showed that our method gives state-of-the-art results.</font> <a href="./VILSS:Material/Teo:Talk.pdf">Slides</a>
</td></tr>
<tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Mar 17
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Edward Jones<br/><i>Dyson Research Lab</i><br/><i>Imperial College London</i><br/><img height="100" alt="Edward" width="75" border="1" src="http://www.doc.ic.ac.uk/~ejohns/Images/photo.png"/>
</td><td valign="top" bgcolor="#f0f0f0">
<b>2D Pairwise Geometry for Robust and Scalable Place Recognition</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>In this talk, I will present an overview of my PhD research on extending recent trends in visual place recognition to offer robustness and scalability. The underlying theme of my work is the exploitation of 2D geometry between pairs of local image features, which is often overlooked in favour of stronger 3D constraints. I will show how 2D geometry can be effectively applied to complement the limitations of 3D geometry, or even replace it at a fraction of the computational cost. The talk is divided into three sections, each discussing one example of such a method. First, I will show how 2D pairwise geometry can help to eliminate false positive feature correspondences which arise from RANSAC-based 3D geometric constraints. Then, an inverted index consisting of pairwise geometries will be introduced, which makes scalable recognition with geometry possible. Finally, I will introduce a topological robot localisation system which aims towards encoding probability into place recognition attempts, and hence offering suitability to visual SLAM frameworks.</font> 
</td></tr>
    <tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Jan 27
    </td><td nowrap valign="top" bgcolor="#f0f0f0">
John Greenall<br/><i>Wirewas</i>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Wirewax - engineering vision algorithms for the wild</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>Wirewax is a platform for turning your videos into rich interactive experiences. Backing the website is a powerful suite of computer vision algorithms that run on a scalable cloud architecture. This talk will detail some of the experiences of training and deploying algorithms for use "in the wild", including discussion of face detection,  recognition and motion tracking.</font> 
</td></tr>
<tr><td align="right" valign="top" bgcolor="#f0f0f0">
2014
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Dec 17
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Zeeshan Zia<br/><i>Dyson Research Lab</i><br/><i>Imperial College London</i><br/><img height="100" alt="Edward" width="75" border="1" src="http://www.igp.ethz.ch/photogrammetry/people/mzia/index.jpg/image"/>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Are Cars Just 3D Boxes?    Jointly Estimating the 3D Shape of Multiple Objects</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>Current systems for scene understanding typically represent objects as 2D or 3D bounding boxes. While these representations have proven robust in a variety of applications, they provide only coarse
approximations to the true 2D and 3D extent of objects. As a result, object-object interactions, such as occlusions or supporting-plane contact, can be represented only superficially. We approach the problem of scene understanding from the perspective of 3D shape modeling, and design a 3D scene representation that reasons jointly about the 3D shape of multiple objects. This representation allows expressing 3D geometry and
occlusion on the fine detail level of individual vertices of 3D wireframe models, and makes it possible to treat dependencies between objects, such as occlusion reasoning, in a deterministic way. The talk will further describe experiments which demonstrate the benefit of jointly estimating the 3D shape of multiple objects in a scene over working with coarse boxes.</font> 
</td></tr>
    
    <tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Nov 18
</td><td nowrap valign="top" bgcolor="#f0f0f0">
Panagiotis Tsakalides<br/><i>Department of Computer Science</i><br/><i>University of Crete</i><br/><img height="100" alt="Edward" width="75" border="1" src="http://www.ics.forth.gr/~tsakalid/PICONS/Tsakalides-photo_2009-03.jpg"/>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Intelligent signal processing and learning in imaging</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>Modern technologies, including the proliferation of high performance
sensors and network connectivity, have revolutionized imaging systems
used in applications ranging from medical and astronomical imaging to
consumer photography. These applications demand even higher speed,
scale, and resolution, which are typically limited by specific imaging
and processing components. While striving for more complex and expensive
hardware is one path, an alternative approach involves the intelligent
design of architectures that capitalize on the advances in cutting edge
signal processing to achieve these goals. This talk will motivate the
need for smart integration of hardware components, on one hand, and
software based recovery, on the other. The talk will showcase the
benefits that stem from Compressed Sensing and Matrix Completion, two
paradigm shifting frameworks in signal processing and learning, in two
imaging problems, namely, range imaging and hyperspectral imaging.
Challenges associated with properties of imaging data such as complexity
and volume will also be presented and viewed under the prism of these
algorithms.</font> 
</td></tr>
    
        <tr><td align="right" valign="top" bgcolor="#f0f0f0">

</td><td nowrap valign="top" bgcolor="#f0f0f0">
Oct 21
</td><td nowrap valign="top" bgcolor="#f0f0f0">
James Charles<br/><i>Department of Computer Science</i><br/><i>University of Leeds</i>
</td><td valign="top" bgcolor="#f0f0f0">
<b>Upper body pose estimation for sign language and gesture recognition</b><br/><font size="2">&nbsp;&nbsp;&nbsp;&nbsp;ABSTRACT:<br>In this talk I present methods for estimating the upper body pose of people performing gestures and sign language in long video sequences. Our methods are based on random forests classifiers and regressors which have proved successful for inferring pose from depth data (Kinect). Here, I will show how we develop methods to: (1) achieve real-time 2D upper body pose estimation without depth data, (2) produce structured pose output from a mixture of random forest experts, (3) use more image context while keeping the learning problem tractable and (4) incorporate temporal context using dense optical flow.</font> 
</td></tr>
    </table>

</body>

</html>
