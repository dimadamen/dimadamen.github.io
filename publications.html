<!DOCTYPE html>
<html>

<head>
    <title>Dima Damen, Professor of Computer Vision, University of Bristol</title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-7551853-2', 'auto');
  ga('send', 'pageview');

</script>
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3{
            color:#006633
        }

    </style>
  </head>
  
  <body>
    <h1>Dima Damen</h1>

    <div class="subheading">
      <img src="Dima2019_s.jpg" />
      Professor of Computer Vision, 
      <a href="http://www.cs.bris.ac.uk">School of Computer Science</a>,<br/>
      Lead of <a href="https://uob-mavi.github.io/">Machine Learning and Computer Vision Group</a>,
      <a href="http://www.bristol.ac.uk">University of Bristol</a>
        <br/>
      EPSRC Early Career Fellow (2020-2025)
        <br/><br/>
      Senior Research Scientist, <a href="https://www.deepmind.com">Google DeepMind</a>
    </div>

    <nav>
      Jump to: 
      <a href="index.html#Projects">Research</a> |
      <a href="publications.html">Publications</a> |
      <a href="about.html">Bio&amp;Group</a> |
      <a href="code.html">Datasets and code</a> |
      <a href="talks.html">Talks</a> |
      <a href="teaching.html">Teaching</a> |
      <a href="contact.html">Contact</a>
    </nav>
      
      <h2>Publications</h2>

      <p><b>Can also be found on <a href="http://scholar.google.co.uk/citations?user=OxL9Wn8AAAAJ&hl=en" target="new">Google Scholar</a></b></p>
      
      <h2>Arxiv</h2>
      
      <p>(2024) T Soucek, P Gatti, M Wray, I Laptev, <u>D Damen</u>, J Sivic. ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions. <a href="https://arxiv.org/abs/2412.01987">ArXiv</a> | <a href="https://soczech.github.io/showhowto/">Website</a> | <a href="https://github.com/soCzech/showhowto">Code</a> | <a href="">Dataset [Coming Soon]</a></p>
      
      <p>(2024) J Huh*, J Chalk*, E Kazakos, <u>D Damen</u>, A Zisserman. EPIC-SOUDNS: A Large-Scale Dataset of Actions that Sound. (Extended Journal Version Under Review) <a href="https://arxiv.org/abs/2302.00646">ArXiv</a></p>
      
      <p>(2024) S Bansal, M Wray, <u>D Damen</u>. HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision. <a href="https://arxiv.org/abs/2404.09933">ArXiv</a> | <a href="https://sid2697.github.io/hoi-ref/">Website</a> | <a href="https://github.com/Sid2697/HOI-Ref/blob/main/hoiqa_dataset/HOIQA_README.md">HOI-QA Dataset</a> | <a href="https://github.com/Sid2697/HOI-Ref">Models and Code</a></p>
            
      <p>(2024) Z Zhu and <u>D Damen</u>. Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos. <a href="https://arxiv.org/abs/2312.15719">ArXiv</a> | <a href="https://zhifanzhu.github.io/getagrip">Website and Videos</a> | <a href="https://github.com/zhifanzhu/getagrip">EPIC-Grasps Dataset and Code</a></p>
      
      <p>(2021) J Munro, M Wray, D Larlus, G Csurka, <u>D Damen</u>. Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval. <a href="https://arxiv.org/abs/2110.12812">ArXiv</a></p>

      <h2>Peer-Reviewed Papers</h2>
      
      <h2>2025</h2>
      
      <p>(2025) C Plizzari, S Goel, T Perrett, J Chalk, A Kanazawa, <u>D Damen</u>. Spatial Cognition from Egocentric Video:
Out of Sight, Not Out of Mind. 3DV <a href="https://arxiv.org/abs/2404.05072">ArXiv</a> | <a href="./OSNOM/">Website</a> | <a href="https://youtu.be/Bo25xedgugY">Video</a></p>  
      
      <p>(2025) A Darkhalil, R Guerrier, A W Harley, <u>D Damen</u> EgoPoints: Optimising Point Tracking for Egocentric Videos. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). [Coming Soon]</p>
      
      <p>(2025) K Flanagan, <u>D Damen</u>, M Wray. Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). [Coming Soon]</p>
      
      <h2>2024</h2>
      
      <p>(2024) T Perrett, T Han, <u>D Damen</u>, A Zisserman. It's Just Another Day: Unique Video Captioning by Discriminative Prompting. Asian Conference on Computer Vision (ACCV). <b>Oral</b> <a href="https://arxiv.org/abs/2410.11702">PDF ArXiv Preprint</a> | <a href="https://tobyperrett.github.io/publications/">Project Webpage</a> | <a href="https://github.com/tobyperrett/its-just-another-day-release">Code and Benchmark</a></p>
      
      <p>(2024) S Sinha, A Stergiou, <u>D Damen</u>. Every Shot Counts: Using Exemplars for Repetition Counting in Videos. Asian Conference on Computer Vision (ACCV). <a href="https://arxiv.org/abs/2403.18074">ArXiv Preprint</a> | <a href="https://sinhasaptarshi.github.io/escounts/">Project Webpage</a> | <a href="https://github.com/sinhasaptarshi/EveryShotCounts">Code</a> | <a href="https://youtu.be/2W5rRGBqdHU">Video</a></p>
      
      <p>(2024) K Grauman et al. Ego4D: Around the World in 3,000 Hours of Egocentric Video. IEEE Transactions of Pattern Analysis and Machine Intelligence. Early Access <a href="https://ieeexplore.ieee.org/document/10611736">PDF</a></p>
      
      <p>(2024) G Goletto, T Nagarajan, G Averta, <u>D Damen</u>. AMEGO: Active Memory from long EGOcentric videos. European Conference on Computer Vision (ECCV). <a href="https://arxiv.org/abs/2409.10917">ArXiv Preprint</a> | <a href="https://gabrielegoletto.github.io/AMEGO/">Webpage</a> | <a href="https://github.com/gabrielegoletto/AMEGO/tree/main/AMB">Benchmark</a> | <a href="https://github.com/gabrielegoletto/AMEGO">Code</a></p> 
      
      <p>(2024) B Zhu, K Flanagan, A Fragomeni, M Wray, <u>D Damen</u>. Video Editing for Video Retrieval. Europen conference on Computer Vision Workshops (ECCVW) <a href="https://arxiv.org/abs/2402.02335">ArXiv</a></p>
      
      <p>(2024) C Plizzari*, G Goletto*, A Furnari*, S Bansal*, F Ragusa*, GM Farinella, <u>D Damen</u>, T Tommasi. An Outlook into the Future of Egocentric Vision. International Journal of Computer Vision (IJCV). <a href="https://rdcu.be/dYfRk">Published Vol 138 IJCV</a> | Accepted - Online May 2024: <a href="https://link.springer.com/content/pdf/10.1007/s11263-024-02095-7.pdf">IJCV PDF</a> | <a href="https://openreview.net/forum?id=V3974SUk1w">OpenReview</a> | <a href="https://arxiv.org/abs/2308.07123">ArXiv</a></p>
            
      <p>(2024) J Chalk, J Huh, E Kazakos, A Zisserman, <u>D Damen</u>. TIM: A Time Interval Machine for Audio-Visual Video Understand. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://jacobchalk.github.io/TIM-Project/">Webpage</a> | <a href="https://github.com/JacobChalk/TIM">Code and Models</a> | <a href="https://arxiv.org/abs/2404.05559">ArXiv</a> | < a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chalk_TIM_A_Time_Interval_Machine_for_Audio-Visual_Action_Recognition_CVPR_2024_paper.pdf">CVF PDF</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chalk_TIM_A_Time_Interval_Machine_for_Audio-Visual_Action_Recognition_CVPR_2024_paper.html">CVF Page</a></p>
      
      <p>(2024) T Soucek, <u>D Damen</u>, M Wray, I Laptev, J Sivic. GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/2312.07322">ArXiv</a> | <a href="https://soczech.github.io/genhowto/">Website</a> | <a href="https://github.com/soczech/GenHowTo">Code</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Souek_GenHowTo_Learning_to_Generate_Actions_and_State_Transformations_from_Instructional_CVPR_2024_paper.html">CVF Open Access</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Souek_GenHowTo_Learning_to_Generate_Actions_and_State_Transformations_from_Instructional_CVPR_2024_paper.pdf">CVF PDF</a></p>
      
      <p>(2024) K Grauman et al. Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="http://arxiv.org/abs/2311.18259">ArXiv</a> | <a href="http://ego-exo4d-data.org">Website and Dataset </a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.html">CVF Open Access</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.pdf">CVF PDF</a></p>
            
      <p>(2024) J Carreira, M King, V Patraucean, D Gokay, C Ionescu, Y Yang, D Zoran, J Heyward, C Doersch, Y Aytar, <u>D Damen</u>, A Zisserman. Learning from One Continuous Video Stream. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/2312.00598">ArXiv</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Carreira_Learning_from_One_Continuous_Video_Stream_CVPR_2024_paper.pdf">CVF PDF</a> | <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Carreira_Learning_from_One_Continuous_Video_Stream_CVPR_2024_paper.html"> CVF Open Access</a></p>
      
      <p>(2024) D Yang, D Tjia, J H Berg, <u>D Damen</u>, P Agrawal, A Gupta. Rank2Reward: Learning Shaped Reward Functions from Passive Video. IEEE International Conference on Robotics and Automation (ICRA). <a href="https://arxiv.org/abs/2404.14735">ArXiv</a> | <a href="https://rank2reward.github.io">Project Webpage and Videos</a></p>
      
      <h2>2023</h2>
      
      <p>(2023) V Tschernezki*, A Darkhalil*, Z Zhu*, D Fouhey, I Laina, D Larlus, <u>D Damen</u>, A Vedaldi. EPIC Fields: Marrying 3D Geometry and Video Understanding. Neural Information Processing Systems (NeurIPS). <a href="https://arxiv.org/abs/2306.08731">Preprint</a> | <a href="http://epic-kitchens.github.io/epic-fields/">Project Webpage</a> | <a href="https://epic-kitchens.github.io/epic-fields/#downloads">Dataset</a> | <a href="https://github.com/epic-kitchens/epic-Fields-code">Code</a> | <a href="https://youtu.be/RcacE26eObE">Video</a> </p> 
      
      <p>(2023) V Patraucean, L Smaira, A Gupta, A Recasens, Y Yang, M Malinowski, C Doersch, L Markeeva, Y Sulsky, D Banarse, S Koppula, T Matejovicova, A Miech, A Frechette, J Zhang, H Klimczak, S Winkler, Y Aytar, R Koster, S Osindero, <u>D Damen</u>, A Zisserman, J Carreira. Perception Test: A Diagnostic Benchmark for Multimodal Models. Neural Information Processing Systems (NeurIPS). <a href="https://arxiv.org/abs/2305.13786">Preprint</a> | <a href="https://github.com/deepmind/perception_test">Dataset and Code</a> | <a href="https://colab.research.google.com/github/deepmind/perception_test/blob/main/inspect_data.ipynb">Colab</a></p>
      
      <p>(2023) K Flanagan, <u>D Damen</u>, M Wray. Learning Temporal Sentence Grounding From Narrated EgoVideos. British Machine Vision Conference (BMVC). <a href="https://arxiv.org/abs/2310.17395">ArXiv Camera Ready</a> | <a href="https://keflanagan.github.io/CliMer-TSG/">Project Webpage</a> | <a href="https://github.com/keflanagan/CliMer">Code and Models</a></p>
      
      <p>(2023) C Plizzari, T Perrett, B Caputo, <u>D Damen</u>. What can a cook in Italy teach a mechanic in India?
Action Recognition Generalisation Over Scenarios and Locations. IEEE/CVF International Conference on Computer Vision (ICCV). <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Plizzari_What_Can_a_ICCV_2023_supplemental.pdf">CVF PDF</a> | <a href="https://arxiv.org/abs/2306.08713">Preprint</a> | <a href="https://chiaraplizz.github.io/what-can-a-cook/">Project Webpage</a> | <a href="https://github.com/Chiaraplizz/ARGO1M-What-can-a-cook#dataset-argo1m">Dataset</a> | <a href="https://github.com/Chiaraplizz/ARGO1M-What-can-a-cook">Code</a> | <a href="https://chiaraplizz.github.io/what-can-a-cook/resources/Supplementary.mp4">Video</a></p>
            
      
      <p>(2023) T Perrett, S Sinha, T Perrett, M Mirmehdi, <u>D Damen</u>. Use Your Head: Improving Long-Tail Video Recognition. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.pdf">CVF PDF</a> | <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Perrett_Use_Your_Head_CVPR_2023_supplemental.pdf">CVF Supp</a> | <a href="https://arxiv.org/abs/2304.01143">ArXiv</a> | <a href="https://github.com/tobyperrett/lmr-release">Benchmark, Code and Models</a> | <a href="https://tobyperrett.github.io/lmr/">Project Webpage</a></p>
      
      <p>(2023) A Stergiou, <u>D Damen</u>. The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf">CVF PDF</a> | <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Stergiou_The_Wisdom_of_CVPR_2023_supplemental.pdf">CVF Supp</a> |  <a href="https://arxiv.org/abs/2204.13340">ArXiv</a> | <a href="https://alexandrosstergiou.github.io/project_pages/TemPr/index.html">Project Webpage</a> | <a href="https://github.com/alexandrosstergiou/progressive-action-prediction">code [Preliminary]</a></p>
      
      <p>(2023) J Huh*, J Chalk*, E Kazakos, <u>D Damen</u>, A Zisserman. EPIC-SOUDNS: A Large-Scale Dataset of Actions that Sound. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). <a href="https://arxiv.org/abs/2302.00646v1">ArXiv Preprint</a> | <a href="https://epic-kitchens.github.io/epic-sounds/">Webpage</a> | <a href="https://github.com/epic-kitchens/epic-sounds-annotations">Dataset, Code and Baseline Models</a> | <a href="https://codalab.lisn.upsaclay.fr/competitions/9729">Audio Recognition Challenge</a></p>
      
      <p>(2023) A Stergiou, <u>D Damen</u>. Play It Back: Iterative Attention for Audio Recognition. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). <a href="https://arxiv.org/abs/2210.11328">ArXiv Preprint</a> | <a href="https://github.com/alexandrosstergiou/PlayItBack">Code</a> | <a href="https://alexandrosstergiou.github.io/project_pages/PlayItBack/index.html">Website</a></p>
      
      <p>(2023) H Wang, M Mirmehdi, <u>D Damen</u>, T Perrett. Centre Stage: Centricity-based Audio-Visual Temporal Action Detection. BMVC Workshops. <a href="https://arxiv.org/abs/2311.16446">ArXiv</a></p>

      
      <h2>2022</h2>
      
       <p>(2022) A Darkhalil, D Shan, B Zhu, J Ma, A Kar, R Higgins, S Fidler, D Fouhey, <u>D Damen</u>. EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations. Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track. <a href="https://arxiv.org/abs/2209.13064">ArXiv</a> | <a href="https://openreview.net/forum?id=djnKHOjpb7I">Paper and Reviews</a> | <a href="https://epic-kitchens.github.io/VISOR/">Project Webpage</a> | <a href="https://epic-kitchens.github.io/VISOR/#downloads">Download</a> | <a href="https://www.youtube.com/watch?v=yGodQAbYW_E">Trailer</a></p>
      
      <p>(2022). A Fragomeni, M Wray, <u>D Damen</u>. ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval. Asian Conference for Computer Vision (ACCV). <b>Accepted as Oral</b>. <a hrerf="https://arxiv.org/abs/2210.04341">ArXiv</a> | <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/ConTra.pdf">PDF Preprint</a> | <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">Project Webpage</a> | <a href="https://github.com/adrianofragomeni/ConTra">Code</a> | <a href="https://youtu.be/vmMJlCAkY7o">Video</a></p>
      
      <p>(2022) K Q Lin, A J Wang, M Soldan, M Wray, R Yan, E Z Xu, D Gao, R Tu, W Zhao, W Kong, C Cai, H Wang, <u>D Damen</u>, B Ghanem, W Liu, M Z Shou. Egocentric Video-Language Pretraining. Neural Information Processing Systems (NeurIPS). <a href="https://arxiv.org/abs/2206.01670">ArXiv Preprint</a> | <a href="https://github.com/showlab/EgoVLP">Code</a></p>

      <p>(2022) W Price, C Vondrick, <u>D Damen</u>. UnweaveNet: Unweaving Activity Stories. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Price_UnweaveNet_Unweaving_Activity_Stories_CVPR_2022_paper.pdf">PDF</a> | <a href="https://arxiv.org/abs/2112.10194">ArXiv Preprint</a> | <a href="https://github.com/willprice/activity-stories">Annotations</a> | <a href="./UnweaveNet/">Project Webpage</a> | <a href="https://youtu.be/Fbod60urVaA">Video</a></p>
      
      <p>(2022) K Grauman et al. Around the World in 3,000 Hours of Egocentric Video. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/2110.07058">ArXiv Preprint</a> | <a href="https://ego4d-data.org/">Ego4D Project and Dataset</a></p>

      
      <p>(2022) J Ma, <u>D Damen</u>. Hand-Object Interaction Reasoning. IEEE Conf. on Advanced Video and Signal-Based Surveillance (AVSS). <a href="https://arxiv.org/abs/2201.04906">Preprint</a></p>
      
      <p>(2022) H. Wang, <u>D Damen</u>, M Mirmehdi, T Perrett. Refining Action Boundaries for One-stage Detection. IEEE Conf. on Advanced Video and Signal-Based Surveillance (AVSS). </p>
      
      <p>(2022) T Perrett, A Masullo, <u>D Damen</u>, T Burghardt, I Craddock, M Mirmehdi. 
Personalized Energy Expenditure Estimation: Visual Sensing Approach With Deep Learning. JMIR Formative Research, vol 5 (9), . <a href="https://formative.jmir.org/2022/9/e33606">PDF</a></p>
      
      <p>(2022) V Popescu, <u>D Damen</u>, T Perrett. An Evaluation of OCR on Egocentric Data. CVPR Workshops. <a href="https://arxiv.org/pdf/2206.05496.pdf">Abstract</a></p>
      
      <p>(2022) D Bazazian, A Calway, <u>D Damen</u>. Dual-Domain Image Synthesis using Segmentation-Guided GAN. IEEE/CVF Computer Vision and Pattern Recognition Workshops (CVPRW). <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Bazazian_Dual-Domain_Image_Synthesis_Using_Segmentation-Guided_GAN_CVPRW_2022_paper.pdf">PDF</a> | <a href="https://arxiv.org/abs/2204.09015">ArXiv</a> | <a href="https://github.com/denabazazian/Dual-Domain-Synthesis">Code</a></p>
      
      <p>(2022) H Wang, <u>D Damen</u>, M Mirmehdi, T Perrett. TVNet: Temporal Voting Network for Action Localization. International Conference on Computer Vision Theory and Applications (VISAPP). <a href="https://arxiv.org/abs/2201.00434">ArXiv</a> | <a href="https://github.com/hanielwang/TVNet">Code</a></p>
      
      <h2>2021</h2>
      
      <p>(2021) E Kazakos, J Huh, A Nagrani, A Zisserman, <u>D Damen</u>. With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition. British Machine Vision Conference (BMVC). <a href="https://arxiv.org/abs/2111.01024">ArXiv</a> | <a href="https://ekazakos.github.io/MTCN-project/">Project and Code</a> | <a href="https://youtu.be/AkvyX79RVvw">Video</a> | <a href="https://github.com/ekazakos/MTCN">Code, features and models</a></p>
      
       <p>(2021) <u>D Damen</u>, H Doughty, GM Farinella, A Furnari, J Ma, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. Rescaling Egocentric Vision: Collection Pipeline and
           Challenges for EPIC-KITCHENS-100. International Journal of Computer Vision (IJCV). (Early Access: <a href="https://link.springer.com/article/10.1007/s11263-021-01531-2">HTML</a> and <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01531-2.pdf">PDF</a>) | <a href="https://arxiv.org/abs/2006.13256">ArXiv (Sep 2021</a>, <a href="https://arxiv.org/abs/2006.13256v1">v1 June 2020)</a> | <a href="epic-kitchens.github.io">Project and Dataset</a></p>
      
      <p>(2021) <u>D Damen</u>, H Doughty, GM Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 43, no. 11, pp. 4125-4141. <a href="https://ieeexplore.ieee.org/document/9084270">IEEE</a> | <a href="https://arxiv.org/abs/2005.00343">Arxiv Preprint</a></p>
      
      <p>(2021) M Wray, H Doughty and <u>D Damen</u>. On Semantic Similarity in Video Retrieval. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wray_On_Semantic_Similarity_in_Video_Retrieval_CVPR_2021_paper.pdf">CVF PDF</a> | <a href="https://arxiv.org/abs/2103.10095">ArXiv Camera Ready</a> | <a href="https://mwray.github.io/SSVR/">Project Details</a> | <a href="https://youtu.be/pS9qa_B771I">Video</a></p>
      
      <p>(2021) T Perrett, T Burghardt, A Masullo, M Mirmehdi and <u>D Damen</u>. Temporal-Relational CrossTransformers for Few-Shot Action Recognition. IEEE/CVF Computer Vision and Pattern Recognition (CVPR). <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf">CVF PDF</a> | <a href="https://arxiv.org/abs/2101.06184">ArXiv Camera Ready</a> | <a href="https://tobyperrett.github.io/trxweb/">Project Details</a> | <a href="https://github.com/tobyperrett/trx">Code</a></p>
      
      <p>(2021) E Kazakos, A Nagrani, A Zisserman, <u>D Damen</u>. Slow-Fast Auditory Streams for Audio Recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). (Accepted) <a href="https://arxiv.org/abs/2103.03516">ArXiv Preprint</a>, <a href="https://ieeexplore.ieee.org/document/9413376">IEEE PDF</a>, <a href="https://github.com/ekazakos/auditory-slow-fast">Code and Models</a>, <a href="https://ekazakos.github.io/auditoryslowfast/">Project Webpage</a> <b style="color:red;">[Outstanding Paper]</b></p>
      
      <p>(2021) B Sullivan, C Ludwig, <u>D Damen</u>, W Mayol-Cuevas, I Gilchrist. Look-Ahead Fixations During Visuomotor Behavior: Evidence from Assembling a Camping Tent. Journal of Vision 21(3):13. <a href="https://jov.arvojournals.org/article.aspx?articleid=2772371">PDF</a></p>
      
      <p>(2021) L Chen, Y Nakamura, K Kondo, <u>D Damen</u>, W Mayol-Cuevas. Integration of Experts’ and Beginners’ Machine Operation Experiences to Obtain a Detailed Task Model. IEICE TRANSACTIONS on Information and Systems. Vol.E104-D(1) Jan 2021, pp 152-161. <a href="https://search.ieice.org/bin/summary.php?id=e104-d_1_152">PDF</a>, <a href="https://www.jstage.jst.go.jp/article/transinf/E104.D/1/E104.D_2019EDP7180/_pdf">Preprint</a></p>
      
      <p>(2021) A Masullo, T Perrett, <u>D Damen</u>, T Burghardt, M Mirmehdi. No Need for a Lab:  Towards Multi-Sensory Fusion for Ambient Assisted Living in Real-World Living Homes. International Conference on Computer Vision Theory and Applications (VISAPP). </p>
      
      <h2>2020</h2>
      
      <p>(2020) W Price, <u>D Damen</u>. Play Fair: Frame Attribution in Video Models. Asian Conference on Computer Vision (ACCV). <a href="https://arxiv.org/abs/2011.12372">ArXiv Preprint</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Price_Play_Fair_Frame_Contributions_in_Video_Models_ACCV_2020_paper.pdf">CVF PDF</a> | <a href="https://play-fair.willprice.dev">Project Details</a> | <a href="http://play-fair.uksouth.cloudapp.azure.com/?uid=137966&n-frames=10" target="_blank">Interactive Dashboard</a> | <a href="https://youtu.be/hBkT_5C7LMQ">Teaser Video </a> | <a href="https://github.com/willprice/play-fair/">Code</a></p>
      
      <p>(2020) T Perrett, A Masullo, T Burghardt, M Mirmehdi, <u>D Damen</u>. Meta-Learning with Context-Agnostic Initialisations. Asian Conference on Computer Vision (ACCV) <a href="https://arxiv.org/abs/2007.14658">ArXiv Preprint</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.pdf">CVF PDF</a> | <a href="https://tobyperrett.github.io/contextagnosticweb/">Project Details</a> | <a href="https://youtu.be/SrksZ-motho">Talk Video</a></p>
      
      
      <p>(2020) J Munro, <u>D Damen</u>. Multi-Modal Domain Adaptation for Fine-Grained Action Recognition. Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/2001.09691">Arxiv (Camera Ready) </a> | <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.pdf"> CVF PDF</a> | <a href="https://jonmun.github.io/mmsada/">Project Details</a> | <a href="https://github.com/jonmun/MM-SADA-code">Code</a> | <a href="https://www.youtube.com/watch?v=ldbPbXYQ5Js&feature=emb_logo">Oral Presentation Video</a> | <a href="https://youtu.be/qgd-DBgf-S0">Results Video</a></p>
      
      <p>(2020) H Doughty, W Mayol-Cuevas, I Laptev, <u>D Damen</u>.  Action Modifiers: Learning from Adverbs in Instructional Videos. Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/1912.06617">Arxiv (Preprint)</a> | <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Doughty_Action_Modifiers_Learning_From_Adverbs_in_Instructional_Videos_CVPR_2020_paper.pdf"> CVF PDF</a> | <a href="https://hazeldoughty.github.io/Papers/ActionModifiers/">Project Details</a> | <a href="https://hazeldoughty.github.io/Papers/ActionModifiers/talk.mp4">Talk Video </a> | <a href="https://youtu.be/rajo0x7WF-c" target="_blank">Results Video</a></p>
      
      <p>(2020) M Lagunes-Fortiz, <u>D Damen</u>, W Mayol. Centroids Triplet Network and Temporally-Consistent Embeddings for In-Situ Object Recognition. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). <a href="./pdfs/Miguel_IROS2020.pdf">Preprint PDF</a></p>
      
      <p>(2020) L Chen, Y Nakamura, K Kondo, <u>D Damen</u>, W Mayol-Cuevas. Integration of Experts' and Beginners' Machine Operation Experiences to Obtain a Detailed Task Model. IEICE TRANSACTIONS on Information and Systems. Vol E104-D (1) - online Jan 2021.</p>
      
      <p>(2020) A Masullo, T Burghardt, <u>D Damen</u>, T Perrett, M Mirmehdi. Person Re-ID by Fusion of Video Silhouettes and Wearable Signals for Home Monitoring Applications. Sensors 29(9) 2576. <a href="https://www.mdpi.com/1424-8220/20/9/2576">PDF</a></p>
 
      <h2>2019</h2>
      
      <p>(2019) M Wray, G Csurka, D Larlus, <u>D Damen</u>. Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings. International Conference on Computer Vision (ICCV). <a href="https://arxiv.org/abs/1908.03477">Arxiv prepring</a> | <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf">CVF PDF</a> | <a href="https://www.youtube.com/watch?v=FLSlRQBFow0">Video</a> | <a href="https://mwray.github.io/FGAR/">Project Details</a></p>
      
      <p>(2019) E Kazakos, A Nagrani, A Zisserman, <u>D Damen</u>. EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition. International Conference on Computer Vision (ICCV). <a href="https://arxiv.org/abs/1908.08498">Arxiv</a> | <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf">CVF PDF</a> | <a href="https://youtu.be/VzoaKsDvv1o" target="_blank">Results Video</a> | <a href="https://www.youtube.com/watch?v=nhUoCbJ3_IQ&feature=youtu.be">Talk Video</a> | <a href="https://ekazakos.github.io/TBN/">Project Details</a></p>
      
      <p>(2019) W Price, <u>D Damen</u>. Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos. ICCV Workshop on Multi-Discipline Approach for Learning Concepts (MDALC). <a href="https://arxiv.org/abs/1909.09422">Arxiv Preprint</a> | <a href="https://video-reversal.willprice.dev">Project Details</a></p>
      
      <p>(2019) F Heidarivincheh, M Mirmehdi, <u>D Damen</u>. Weakly-Supervised Completion Moment Detection using Temporal Attention. ICCV Workshop on Human Behaviour Understanding. <a href="https://arxiv.org/abs/1910.09920">Arxiv</a> | <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/HBU/Heidarivincheh_Weakly-Supervised_Completion_Moment_Detection_using_Temporal_Attention_ICCVW_2019_paper.pdf">CVF PDF</a></p>
      
      <p>(2019) A Masullo, T Burghardt, <u>D Damen</u>, T Perrett, M Mirmehdi. Who Goes There? Exploiting Silhouettes and Wearable Signals for Subject Identification in Multi-Person Environments. ICCV Workshop on Computer Vision for Physiological Measurement.</p>
      
      <p>(2019) M Wray, <u>D Damen</u>. Learning Visual Actions Using Multiple Verb-Only Labels. British Machine Vision Conference (BMVC). <a href="https://arxiv.org/abs/1907.11117">Arxiv Preprint</a> | <a href="https://mwray.github.io/MVOL/MVOL.pdf">PDF</a> | <a href="https://www.youtube.com/watch?v=GEJRi5etiaE">Video</a> | <a href="https://mwray.github.io/MVOL/">Project Details</a></p>
      
      <p>(2019) T Perrett, <u>D Damen</u>. DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition. Computer Vision and Pattern Recognition (CVPR). <a href="https://tobyperrett.github.io/ddlstmweb/main.pdf">Prepring</a> | <a href="https://arxiv.org/abs/1904.08634">Arxiv</a> | <a href="https://youtu.be/8MtC6X4w4jE">Video</a> | <a href="https://tobyperrett.github.io/ddlstmweb/">Project Details</a></p>
      
      <p>(2019) D Moltisanti, S Fidler, <u>D Damen</u>. Action Recognition from Single Timestamp Supervision in Untrimmed Videos. Computer Vision and Pattern Recognition (CVPR).<a href="single_timestamps/index.html">Project Details</a> | <a href="single_timestamps/paper/Action_Recognition_with_Single_Timestamp_Supervision__CVPR_.pdf">PDF (preprint)</a> | <a href="https://arxiv.org/abs/1904.04689">Arxiv</a> | <a href="https://bitbucket.org/dmoltisanti/action_recognition_single_timestamps/src">Code</a></p>
      
      <p>(2019) H Doughty, W Mayol-Cuevas, <u>D Damen</u>. The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos. Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/1812.05538">arxiv</a> | <a href="./TheProsandCons/index.html">Project Details</a></p>
      
      <p>(2019) M Lagunes-Fortiz, W Mayol-Cuevas, <u>D Damen</u>. Learning Discriminative Embeddings for
Object Recognition on-the-fly. International Conference on Robotics and Automation (ICRA) <a href="pdfs/ICRA2019_camera_ready.pdf">PDF (preprint)</a></p>
      
      <p>(2019) Y Jang, B Sullivan, C Ludwig, I.D. Gilchrist, <u>D Damen</u>, W Mayol-Cuevas. EPIC-Tent: An Egocentric Video Dataset for Camping Tent Assembly. International Conference on Computer Vision Workshop. <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.pdf">PDF</a> | <a href="https://sites.google.com/view/epic-tent">Project Details</a> | <a href="https://www.google.com/url?q=https%3A%2F%2Fdata.bris.ac.uk%2Fdata%2Fdataset%2F2ite3tu1u53n42hjfh3886sa86&sa=D&sntz=1&usg=AFQjCNFqid3XlHOLRR1zaw4gSHFTq_VmwA">Dataset</a> | <a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fyoungkyoonjang%2FEPIC_Tent2019&sa=D&sntz=1&usg=AFQjCNF_rH_7LMUSGmkuDi8pzEboQgEALQ">Annotations</a> | <a href="https://youtu.be/MGqT6J6JJ4I">Video</a></p>
      
      <p>(2019) L Chen, Y Nakamura, K Kondo, <u>D Damen</u>, W Mayol-Cuevas. Hotspots Integrating of Expert and Beginner Experiences of Machine OperatiFons through Egocentric Vision. Machine Vision and Applications (MVA).</p>
      
      <p>(2019) A Elkholy, M Hussein, W Gomaa, <u>D Damen</u>, E Saba. Efficient and Robust Skeleton-Based Quality Assessment and Abnormality Detection in Human Action Performance. IEEE Journal of Biomedical and Health Informatics <a href="https://ieeexplore.ieee.org/document/8664567/">PDF (Early Access)</a></p>
      
      <p> (2019) A Masullo,T Burghardt, T Perrett, <u>D Damen</u>, Majid Mirmehdi. Sit-to-Stand Analysis in the Wild using Silhouettes for Longitudinal Health Monitoring. International Conference on Image Analysis and Recognition (ICIAR). <a href="https://arxiv.org/abs/1910.01370">ArXiv Preprint</a></p>
      
      <p>(2019) V Ponce-López, T Burghardt, Y Sun, S Hannuna, <u>D Damen</u>, M Mirmehdi. Deep Compact Person Re-Identification with Distractor Synthesis via Guided DC-GANs. International Conference on Image Analysis and Processing (ICIAP). <a href="https://link.springer.com/chapter/10.1007/978-3-030-30642-7_44">PDF</a></p>
      
      <p> (2019) B Sullivan, H Doughty, W Mayol-Cuevas, <u>D Damen</u>, C Ludwig, I Gilchrist. [ABSTRACT:] Detecting Uncertainty While Assembling a Camping Tent. European Conference on Visual Perception (ECVP). </p>
      
      <h2>2018</h2>
      
      <p>(2018) <u>D Damen</u>, H Doughty, GM Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. Scaling Egocentric Vision: The EPIC-KITCHENS Dataset. European Conference on Computer Vision (ECCV). <a href="http://arxiv.org/abs/1804.02748">arxiv</a> | <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html">CVF PDF</a> | <a href="http://epic-kitchens.github.io/">Dataset</a> | <a href="http://epic-kitchens.github.io">Project Page</a></p>
      
      <p>(2018) F Heidarivincheh, M Mirmehdi, <u>D Damen</u>. Action Completion: A Temporal Model for Moment Detection. British Machine Vision Conference (BMVC). <a href="https://arxiv.org/abs/1805.06749">Arxiv</a> | <a href="https://github.com/FarnooshHeidari/CompletionDetection">Dataset</a> | <a href="./ActionCompletion/">Project Page</a></p>
      
      <p>(2018) A Masullo, T Burghardt, <u>D Damen</u>, S Hannuna, V Ponce-López, M Mirmehdi. CaloriNet: From silhouettes to calorie estimation in private environments. British Machine Vision Conference (BMVC). <a href="https://arxiv.org/abs/1806.08152">Arxiv</a></p>
      
      <p>(2018) H Doughty, <u>D Damen</u>, W Mayol-Cuevas. Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination. Computer Vision and Pattern Recognition (CVPR). <a href="https://arxiv.org/abs/1703.09913">arxiv</a> | <a href="./Skill/">Project Page</a> | <a href="https://github.com/hazeld/EPIC-Skills2018">Dataset</a></p>
      
      <p>(2018) Y Xu, <u>D Damen</u>. Human Routine Change Detection using Bayesian Modelling. International Conference on Pattern Recognition (ICPR) <a href="Routine/ICPR18_XuDamen.pdf">Preprint</a> | <a href="./Routine/">Project Page</a></p>
      
      <p>(2018) M. Lagunes-Fortiz, <u>D Damen</u>, W Mayol-Cuevas. Instance-level Object Recognition on Video Data using Deep Temporal Coherence. International Symposium on Visual Computing (ISVC).</p>
      
      <p>(2018) V Ponce-López, T Burghardt, S Hannunna, <u>D Damen</u>, A Masullo, M Mirmehdi. Semantically selective augmentation for deep compact person re-identification. European Conference on Computer Vision Workshops (ECCVW). <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf">PDF CVF</a></p>
      
      <p>(2018) V Soleimani, M Mirmehdi, <u>D Damen</u>, J Dodd. Markerless Active Trunk Shape Modelling for Motioin Tolerant Remote Respiratory Assessment. International Conference on Image Processing (ICIP). </p>
      
      <p>(2018) V Soleimani, M Mirmehdi, <u>D Damen</u>, James Dodd, Massimo Camplani, Sion Hannuna, Charlie Sharp, Jason Viner. Depth-based Whole Body Photoplethysmography in Remote Pulmonary Function Testing. IEEE Transactions on Biomedical Engineering, vol 65(6), pp 1421 - 1431.</p>
          
      <p>(2018) L Tao, T Burghardt, M Mirmehdi, <u>D Damen</u>, Ashley Cooper, Sion Hannuna, Massimo Camplani, Adeline Paiment, I Craddock. Energy Expenditure Estimation using Visual and Intertial Sensors. IET Computer Vision vol 12 (1) pp 36 - 47</p>
      
      
      <p>(2018) F De Luca, <u>D Damen</u>, J Kurton, M Wray, RM Pokhrel, MJ Werner. Traffic data as proxy of business downtime after natural disasters: the case of Kathmandu. National Conference on Earthquake Engineering. <a href="https://core.ac.uk/download/pdf/200761639.pdf">PDF</a></p>
      
      <h2>2017</h2>
      <p>(2017) D Moltisanti, M Wray, W Mayol-Cuevas, <u>D Damen</u>. Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video. International Conference on Computer Vision (ICCV). <a href="./Trespass/trespassingBoundariesLabeling.pdf">pdf (camera ready)</a> | <a href="https://arxiv.org/abs/1703.09026">arxiv</a> | <a href="./Trespass/">Project Page</a> | <a href="https://youtu.be/mWr8JJDgA3w">video</a></p>
      
      <p>(2017) T Perrett, <u>D Damen</u>. Recurrent Assistance: Cross-Dataset Training of LSTMs on Kitchen Tasks. Fifth Int. Workshop on Assistive Computer Vision and Robotics (ACVR). International Conference on Computer Vision Workshops (ICCVW). <a href="./LSTMT/acvr2017.pdf">pdf (camera ready)</a></p>
      
      <p>(2017) R Layne, S Hannuna, M Camplani, J Hall, T Hospedales, T Xiang, M Mirmehdi, <u>D Damen</u>. A Dataset for Persistent Multi-Target Multi-Camera Tracking in RGB-D. IEEE Computer Vision and Pattern Recognition Workshops (CVPRW) <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Layne_A_Dataset_for_CVPR_2017_paper.pdf">pdf</a></p>
      
    <p>(2017) V Soleimani, M Mirmehdi, <u>D Damen</u>, S Hannuna, M Camplani. Remote, Depth-based Lung Function Assessment. IEEE Transactions on Biomedical Engineering, vol 64(8) pp 1943 - 1958 <a href="http://ieeexplore.ieee.org/document/7763855/">pdf</a></p>
      
      <p>(2017) C Sharp, V Soleimani, S Hannuna, M Camplani, <u>D Damen</u>, J Viner, M Mirmehdi, and J Dodd. Toward Respiratory Assessment Using Depth Measurements from a Time-of-Flight Sensor. Fronteirs in Physiology 8:65. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5293747/">pdf</a></p>
      
      <p>(2017) M Camplani, A Paiement, M Mirmehdi , <u>D Damen</u>, S Hannuna, T
Burghardt, L Tao. Multiple human tracking in RGB-depth data: a
survey. IET Computer Vision, vol 11 (4) pp 265-285 <a href="http://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2016.0178">pdf</a> | <a href="https://arxiv.org/abs/1606.04450">ArXiv</a></p>
      
      <p>(2017) T Leelasawassuk, <u>D Damen</u>, W Mayol-Cuevas. Automated capture and delivery of assistive task guidance with an eyewear computer: The GlaciAR system. Augmented Human. <a href="http://dl.acm.org/citation.cfm?id=3041185">pdf</a> | <a href="https://arxiv.org/abs/1701.02586">ArXiv</a> | <a href="https://www.youtube.com/watch?v=USFaDjXkPfs">video</a></p>
      
      <p>(2017) Y Xu, D Bull, <u>D Damen</u>. Unsupervised Long-Term Routine Modelling using Dynamic Bayesian Networks. IEEE Int Conf on Digital Image Computing Technologies and Applications (DICTA). <a href="Routine/DICTA17_XuBullDamen.pdf">PDF</a></p>
      
      <p>(2017) L Chen, K Kondo, Y Nakamura, <u>D Damen</u>, W Mayol-Cuevas. Hotspots Detection for Machine Operation in Egocentric Vision. Machine Vision Applications (MVA) <a href="http://ieeexplore.ieee.org/abstract/document/7986841/">pdf</a> (TBA), <a href="https://youtu.be/ZADxABLz1GM">video</a></p>
      
      <p>(2017) S Audrey, U Leonard, <u>D Damen</u>, Shared Use Routes for People Who Walk or Cycle: Addressing the Challenges. International Conference for Transport and Health, vol 5, pp 57-58 (abstract)</p>
      
      <p>(2017) A Elkholy, M Hussein, W Gomaa, <u>Dima Damen</u>, Emmanuel Saba. A general descriptor for detecting abnormal action performance from skeletal data. IEEE Engineering in Medicine and Biology Society (EMBC). <a href="http://ieeexplore.ieee.org/abstract/document/8037095/">pdf</a></p>
      
      
      
<h2>2016</h2>

      <p>(2016) <u>D Damen</u>, T Leelasawassuk, W Mayol-Cuevas. You-Do, I-Learn: Egocentric Unsupervised Discovery of Objects and their Modes of Interaction Towards Video-Based Guidance. Computer Vision and Image Understanding (CVIU), vol 149 pp 98-112 August 2016. [<a href="http://dx.doi.org/10.1016/j.cviu.2016.02.016">pdf</a> | <a href="http://arxiv.org/abs/1510.04862v2">arxiv preprint</a>]</p>
      
            <p>(2016) L Tao, A Paiment, <u>D Damen</u>, M Mirmehdi, S Hannuna, M Camplani, T Burghardt, I Craddock. A Comparative Study of Pose Representation and Dynamics Modelling for Online Motion Quality Assessment. Computer Vision and Image Understanding (CVIU), vol 148 pp 136-152 July 2016. [<a href="http://dx.doi.org/10.1016/j.cviu.2015.11.016">pdf</a> | <a href="http://www.irc-sphere.ac.uk/lib/tinymce/plugins/moxiemanager/data/files/QualityAssess-Final.pdf">Preprint</a>]</p>

            <p>(2016) F Heidarivincheh, M Mirmehdi, <u>D Damen</u>. Beyond Action Recognition: Action Completion in RGB-D Data. British Machine Vision Conference (BMVC). <a href="./ActionCompletion/ActionCompletion_BMVC2016.pdf">pdf</a> | <a href="./ActionCompletion/ActionCompletion_BMVC2016_abstract.pdf">abstract</a> | <a href="https://youtu.be/iBdW-kVKMds">video</a> | <a href="./ActionCompletion/">project</a> | <a href="http://dx.doi.org/10.5523/bris.66qry08cv1fj1eunwxwob3fjz">dataset</a></p>
      
      <p>(2016) V Soleimani, M Mirmehdi, <u>D Damen</u>, S Hannuna, M Camplani. 3D Data Acquisition and Registration using Two Opposing Kinects. 3D Vision (3DV). <a href="http://ieeexplore.ieee.org/document/7785085/">pdf</a> | <a href="https://github.com/BristolVisualPFT/3D_Data_Acquisition_Registration_Using_Kinects/tree/master/Double_opposing_Kinects">code</a></p>
      
    
<p>(2016) M Wray, D Moltisanti, W Mayol-Cuevas, <u>D Damen</u>. SEMBED: Semantic Embedding of Egocentric Action Videos. First International Workshop on Egocentric Percetion, Interaction and Computing (EPIC). European Conference on Computer Vision Workshops (ECCVW). <a href="./SEMBED/ECCVW2016SEMBED.pdf">pdf</a> | <a href="./SEMBED/ECCVW2016SEMBEDSupp.pdf">supplementary</a> | <a href="http://youtu.be/6bDDTIJUuic">video</a> | <a href="./BEOID/">dataset</a> | <a href="./SEMBED">project</a></p>

     <p>(2016) L Tao, T Burghardt, S Hannuna, M Camplani, A Paiement, <u>D Damen</u>, M Mirmehdi, I Craddock. Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home. 13th Asian Conference on Computer Vision (ACCV 2016) Workshop on Assistive Vision. <a href="https://arxiv.org/pdf/1607.08196.pdf">ArXiv</a> | <a href="http://www.irc-sphere.ac.uk/work-package-2/calorie">project</a>
      
     
      <h2>2015</h2>
<p>(2015) G Bleser, <u>D Damen</u>, A Behera, G Hendeby, K Mura, M Miezal, A Gee, N Petersen, G Macaes, H Domingues, D Gorecky, L Almeida, W Mayol-Cuevas, A Calway, A Cohn, D Hogg, D Stricker. Cognitive Learning, Monitoring and Assistance of Industrial Workflows Using Egocentric Sensor Networks. PLOS ONE, 30 June 2015. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127769">pdf</a></p>
      
      <p>(2015) M Camplani, S Hannuna, M Mirmehdi, <u>D Damen</u>, L Tao, T Burghardt, A Paiement. Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling. British Machine Vision Conference (BMVC). <a href="http://bmvc2015.swansea.ac.uk/proceedings/papers/paper145/paper145.pdf">pdf</a> | <a href="http://bmvc2015.swansea.ac.uk/proceedings/papers/paper145/abstract145.pdf">abstract</a> | <a href="https://github.com/mcamplan/DSKCF_CPP">code</a> | <a href="http://www.irc-sphere.ac.uk/work-package-2/DS-KCF">project</a></p>
      
      <p>(2015) T Leelasawassuk, <u>D Damen</u>, W Mayol-Cuevas. Estimating Visual Attention from a Head Mounted IMU. International Symposium on Wearable Computers (ISWC). <a href="http://www.cs.bris.ac.uk/Publications/Papers/2001754.pdf">pdf</a> <a href="https://youtu.be/DTgWpzIlZ_M">video</a></p>
      
      <p>(2015) V Soleimani, M Mirmehdi, <u>D Damen</u>, S Hannuna, M Camplani, J Vinery, J Boddy. Remote Pulmonary Function Testing using a Depth Sensor. IEEE/CAS-EMB Biomedical Circuits and Systems Conference (BioCAS). <a href="http://ieeexplore.ieee.org/document/7348445/">pdf</a> | <a href="https://youtu.be/NVCnJmQIvdU">video</a></p>
      
      <p>(2015) Y Xu, D Bull, <u>D Damen</u>. Unsupervised Daily Routine Modelling from a Depth Sensor using Bottom-Up and Top-Down Hierarchies. Asian Conference on Pattern Recognition (ACPR). <a href="http://ieeexplore.ieee.org/document/7486465/">pdf</a></p>
      
      <p>(2015) T Hodan, <u>D Damen</u>, W Mayol-Cuvas, J Matas. Efficient Texture-less Object Detection for Augmented Reality Guidance. Workshop on Visual Recognition and Retrieval for Mixed and Augmented Reality. IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR) Workshop. <a href="http://cmp.felk.cvut.cz/~hodanto2/data/hodan2015efficient.pdf">pdf</a></p>
      
      <p>(2015) L Tao, T Burghardt, S Hannuna, M Camplani, A Paiment, <u>D Damen</u>, M Mirmehdi, I Craddock. A Comparative Home Activity Monitoring Study using Visual and Inertial Sensors. IEEE Int. Conf. on e-Health Networking, Applications and Services (Healthcom)</p>
      
      <p>(2015) P Woznowski, X Fafoutis, T Song, S Hannuna, M Camplani, L Tao, A Paiement, E Mellios, M Haghighi, N Zhu, G Hilton, <u>D Damen</u>, T Burghardt, M Mirmehdi, R Piechocki, D Kaleshi, I Craddock. A Multi-modal Sensor Infrastructure for Healthcare in a Residential Environment. IEEE ICC Workshop on ICT-enabled services and technologies for eHealth and Ambient Assisted Living</p>
      
      <h2>2014</h2>
      
      <p>(2014) <u>D Damen</u>, T Leelasawassuk, O Haines, A Calway, W Mayol-Cuevas. You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video. British Machine Vision Conference (BMVC), Nottingham, UK. <a href="You-Do-I-Learn/Damen_BMVC2014.pdf">pdf</a> | <a href="You-Do-I-Learn/Damen_BMVC2014_abstract.pdf">abstract</a> | <a href="https://youtu.be/vUeRJmwm7DA">video</a> | <a href="https://www.cs.bris.ac.uk/~damen/BEOID/">dataset</a> | <a href="https://www.cs.bris.ac.uk/~damen/You-Do-I-Learn">project</a></p>
      
      <p>(2014) A Paiment, L Tao, S Hannuna, M Camplani, <u>D Damen</u>, M Mirmehdi. Online quality assessment of human movement from skeleton data. British Machine Vision Conference (BMVC), Nottingham, UK. <a href="Sphere/bmvc14.pdf">pdf</a> | <a href="Sphere/bmvc14_abstract.pdf">abstract</a> | <a href="http://www.irc-sphere.ac.uk/work-package-2/movement-quality">project and datasets</a></p>
      
      <p>(2014) <u>D Damen</u>, O Haines, T Leelasawassuk, A Calway, W Mayol-Cuevas. Multi-user egocentric Online System for Unsupervised Assistance on Object Usage. Computer Vision - ECCV 2014 Workshops Proceedings - Part III, p. 481-492, Zurich, Switzerland. <a href="You-Do-I-Learn/Damen_ACVR2014_prePrint.pdf">preprint</a></p>

      <h2>2012</h2>
      <p>(2012) <u>D Damen</u>, D Hogg. Detecting Carried Objects from Sequences of Walking Pedestrians. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) vol 34 (6) pp 1056-1067 <a href="http://dx.doi.org/10.1109/TPAMI.2011.205">pdf</a> | <a href="http://www.comp.leeds.ac.uk/vision/BaggageDetection/baggage.htm">project</a> | <a href="https://www.youtube.com/watch?v=ZFPhr7mx4Mw">video</a></p>
<p>(2012) <u>D Damen</u>, D Hogg. Explaining Activities as Consistent Groups of Events - A Bayesian Framework using Attribute Multiset Grammars. International Journal of Computer Vision (IJCV) vol 98 (1) pp 83-102. <a href="http://www.springerlink.com/content/x4vm7237u646u523/">pdf</a> | <a href="http://www.comp.leeds.ac.uk/vision/Bicycles/bikes2.htm">project</a></p>
      
      <p>(2012) <u>D Damen</u>, P Bunnun, A Calway, W Mayol-Cuevas. Real-time Learning and Detection of 3D Texture-less Objects: A Scalable Approach. British Machine Vision Conference (BMVC) <a href="bmvc2012_scalable_textureless.pdf">pdf</a> | <a href="bmvc2012_abstract.pdf">abstract</a> | <a href="bmvc2012_poster_small.png">poster</a> | <a href="MultiObjDetector.htm">code</a> <font color="#0000DD"><b>[*Best Poster Prize*]</b></font></p>
      
      
<p>(2012) <u>D Damen</u>, A Gee, W Mayol-Cuevas, A Calway. Egocentric Real-time Workspace Monitoring using an RGB-D Camera. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) <a href="Egocentric_IROS2012.pdf">pdf</a></p>
      
      <p>(2012) P Bunnun, <u>D Damen</u>, A Calway, W Mayol-Cuevas. Integrating 3D Object Detection, Modelling and Tracking on a Mobile Phone. International Symposium on Mixed and Augmented Reality (ISMAR) </p>
      
      <h2>2011</h2>
      
      <p>(2011) <u>D Damen</u>, A Gee, A Calway, W Mayol-Cuevas. Detecting and Localising Multiple 3D Objects: A Fast and Scalable Approach. IROS Workshop on Active Semantic Perception and Object Search in the Real World (ASP-AVS-11) <a href="IROS-W2011.pdf">pdf</a></p>
      
      <h2>2010</h2>
            <p>(2010) P Bunnun, <u>D Damen</u>, S Subramanian, W Mayol-Cuevas. Interactive Image-Based Model Building for Handheld Devices. ISMAR Workshop on Augmented Reality Super Models <a href="ISMAR-W2010.pdf">pdf</a></p>
      <h2>2009</h2>
      
      <p>(2009) <u>D Damen</u>, D Hogg. Attribute Multiset Grammars for Global Explanations of Activities. British Machine Vision Conference (BMVC). <a href ="Damen_BMVC09.pdf">pdf</a> | <a href="Damen_BMVC09_abstract.pdf">abstract</a></p>

<p>(2009) <u>D Damen</u>, D Hogg. Recognizing Linked Events: Searching the Space of Feasible Explanations. Computer Vision and Pattern Recognition (CVPR) <a href="CVPR09.pdf">pdf</a> | <a href="CVPR09_poster.jpg">poster</a></p>

      <h2>2008</h2>
      
      <p>(2008) <u>D Damen</u>, D Hogg. Detecting Carried Objects in Short Video Sequences. European Conference on Computer Vision (ECCV) Springer-Verlag. 
<b>3</b>,154-167 <a href="DamenHoggECCV2008.pdf">pdf</a> <a href="http://www.comp.leeds.ac.uk/vision/BaggageDetection/ECCV08P.gif" target="_new">poster</a> <a href="ECCVDemo.avi">demo</a></p>

      <h2>2007</h2>


<p>(2007) <u>D Damen</u>, D Hogg. Associating People Dropping off and
Picking up Objects. British Machine Vision Conference (BMVC). <a href="DamenHoggBMVC07.pdf">pdf</a> <a href="BMVCPresentation.pdf">Oral Presentation</a></p>

<p>(2007) <u>D Damen</u>, D Hogg. Bicycle Theft Detection.
International Crime Science Conference. (CS2) <a href="DamenHoggCS207.pdf">pdf</a> <a href="IC2Presentation.pdf">Oral Presentation</a></p>

<p>(2007) <u>D Damen</u>, D Hogg. <a href="makeSomeNoisePoster.htm"
target="_self">Bicycle Theft Detection - How to deal with visual uncertainties</a>.
Make Some Noise (Faculty of Engineering Postgraduate Research Symposium).
Faculty of Engineering, University of Leeds</p>

<h2>Technical Reports</h2>
      
            
      <p>(2021) <u>D Damen</u>, A Fragomeni, J Munro, T Perrett, D Whettam, M Wray, A Furnari, G M Farinella, D Moltisanti. EPIC-KITCHENS-100- 2021 Challenges Report. <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">PDF</a></p>
      
      <p>(2020) <u>D Damen</u> and M Wray. Supervision Levels Scale (SLS). <a href="https://arxiv.org/abs/2008.09890">ArXiv</a></p>
     
      <p>(2020) <u>D Damen</u>, E Kazakos, W Price, J Ma, H Doughty, A Furnari, GM Farinella. EPIC-KITCHENS-55 - 2020 Challenges Report. <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2020-Report.pdf">PDF</a></p>
      
      <p>(2019) W Price, <u>D Damen</u>. An Evaluation of Action Recognition Models on EPIC-Kitchens. <a href="https://arxiv.org/abs/1908.00867">Arxiv</a> | <a href="https://github.com/epic-kitchens/action-models">Github</a> | <a href="EPICModels/price_epic_models2019.pdf">PDF</a></p>
      
      <p>(2019) <u>D Damen</u>, W Price, E Kazakos, A Furnari, GM Farinella. EPIC-KITCHENS - 2019 Challenges Report. <a href="https://epic-kitchens.github.io/Reports/EPIC-Kitchens-Challenges-2019-Report.pdf">PDF</a></p>


<p>(2016) S Gunner, <u>D Damen</u>. <a href="http://research-information.bristol.ac.uk/en/publications/potential-computer-vision-technologies-for-monitoring-shared-spaces-bristol-case-study(9cdc7699-e985-45d7-96a9-c6df9c52a06f).html">Potential Computer Vision Technologies for Monitoring Shared Spaces (Bristol Case Study)</a>. Commissioned by the Cabot Institute, University of Bristol Technical Reports</p>
<p>(2013) G Bleser, L Almeida, A Behera, A Calway, A Cohn, <u>D Damen</u>, H Domingues, A Gee, D Gorecky, D Hogg, M Kraly, G Macaes, F Marin, W Mayol-Cuevas, M Miezal, K Mura, N Petersen, N Vignais, L Paulo Santos, G Spaas, D Stricker. <a href="http://www.dfki.de/web/forschung/av/publikationen?pubid=6725">Cognitive Workflow Capturing and Rendering with On- Body Sensor Networks (COGNITO)</a>.
German Research Center for Artificial Intelligence, DFKI Research Reports (RR), Vol. 13-02.

<h2>Editorial Work</h2>
<p>Editors: Nalpantidis, Lazaros and Detry, Renaud and Damen, Dima and Bleser, Gabriele and Cakmak, Maya and Suphi Erden, Mustafa. Cognitive Robotics Systems: Concepts and Applications. Journal of Intelligent & Robotic Systems (June 2015). DOI: <a href="http://dx.doi.org/10.1007/s10846-015-0244-9"> 10.1007/s10846-015-0244-9</a></p>
    
<p>Editors: Burghardt, Tilo and Damen, Dima and Mayol-Cuevas, Walterio and Mirmehdi, Majid. Correspondence, Matching and Recognition. International Journal of Computer Vision - Special Issue (May 2015) DOI:<a href="http://dx.doi.org/10.1007/s11263-015-0827-8">10.1007/s11263-015-0827-8</a>
<p>Editors: Burghardt, Tilo and Damen, Dima and Mayol-Cuevas, Walterio and Mirmehdi, Majid. Proceedings of the British Machine Vision Conference 2013. British Machine Vision Association (Bristol, September 2013).

    
<h2>Book Chapters</h2>
<p>(2016) Woznowski et al. SPHERE: A Sensor Platform for Healthcare in a Residential Environment. Designing, Developing, and Facilitating Smart Cities. <a href="http://link.springer.com/chapter/10.1007/978-3-319-44924-1_14">pdf</a></p>

<h2> Theses</h2>
<p>(2009) Activity Analysis: Finding Explanations for Sets of Events. <i>PhD Thesis</i>. University of Leeds <a href="http://etheses.whiterose.ac.uk/1368/1/DimaDamen.pdf">pdf (6MB)</a></p>
<p>(2003) Visual Signature for Large Scale Tracking. <i>MSc Thesis</i>. University of Leeds <a href="aldamen.pdf">pdf</a></p>

</body>

</html>
