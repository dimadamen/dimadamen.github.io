<!DOCTYPE html>
<html>
  <head>
    <title>Dima Damen, Professor of Computer Vision, University of Bristol</title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
        <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-7551853-2', 'auto');
      ga('send', 'pageview');
            
            // widgets script loading taken from Twitter
window.twttr = (function (d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0],
        t = window.twttr || {};
    if (d.getElementById(id)) return;
    js = d.createElement(s);
    js.id = id;
    js.src = "https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js, fjs);

    t._e = [];
    t.ready = function (f) {
        t._e.push(f);
    };

    return t;
}(document, "script", "twitter-wjs"));

twttr.ready(function (twttr) {
    twttr.events.bind("rendered", function (event) {
        var widgetFrame = event.target;
        var retweets = widgetFrame.contentDocument.querySelectorAll('div.timeline-Tweet--isRetweet');
        retweets.forEach(function (node) {
                if (node.parentNode && node.parentNode.style !== 'display: none;') { // (in)sanity check
                    node.parentNode.style = 'display: none;' // hide entire parent li tag
                }
        })
    });
});

    </script>
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3{
            color:#006633
        }

    </style>
  </head>
  
  <body>
   <table cellspacing="0"><tr><td width=70%>
    <h1>Dima Damen</h1>
    <div class="subheading">
      <img src="Dima2019_s.jpg"/>
      Professor of Computer Vision, 
      <a href="http://www.cs.bris.ac.uk">Department of Computer Science</a>,<br/>
      <a href="http://www.bris.ac.uk/vi-lab/">Visual Information Laboratory</a>,
      <a href="http://www.bristol.ac.uk">University of Bristol</a>
        <br/><br/>
      EPSRC Early Career Fellow (2020-2025)
        
        <p>
        <a href="http://twitter.com/dimadamen">
                            <a class="twitter-follow-button" href="http://twitter.com/dimadamen" data-size="large">
                                Follow @dimadamen</a></a></p>
        
    </div>

    <nav>
      Jump to: 
      <a href="index.html#Projects">Research</a> |
      <a href="publications.html">Publications</a> |
      <a href="about.html">Bio&amp;Group</a> |
      <a href="code.html">Datasets and code</a> |
      <a href="talks.html">Talks</a> |
      <a href="teaching.html">Teaching</a> |
      <a href="contact.html">Contact</a>
    </nav>
    
       <h2>News</h2>
                <p>Nov 2021: Our BMVC 2021 paper "With a little help from my temporal context" is now <a href="https://arxiv.org/abs/2111.01024">Available on ArXiv</a></p>
                <p>Oct 2021: <a href="https://ego4d-data.org/">Ego4D Project</a> and <a href="https://arxiv.org/abs/2110.07058">ArXiv paper</a> now out. <a href="https://www.youtube.com/watch?v=2dau0W0NVQY">Reveal session during EPIC@CCV2021</a></p>
                <p>Sep 2021: Our paper <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01531-2.pdf">Rescaling Egocentric Vision</a> accepted at IJCV</p>
                <p>Aug 2021: Welcoming <a href="https://alexandrosstergiou.github.io/">Alexandros Stergiou</a> and <a href="https://binzhubz.github.io/">Bin Zhu</a> as postdocs to the group</p>
                <p>July 2021: <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">Technical report for 2021 EPIC-KITCHENS challengs</a> is online</p>
   <p><a id="showOlderNews" href="#olderNews">Older news...</a></p>
    <div id="olderNews">
      <a name="olderNews" />
                <p>June 2021: Our paper <a href="https://ekazakos.github.io/auditoryslowfast/">Slow-Fast Auditory Streams</a> won outstanding paper at ICASSP 2021 (3 out of 1700 papers awarded)</p>
                <p>June 2021: <a href="https://www.robots.ox.ac.uk/~vgg/projects/visualai/">EPSRC Program Grant Visual AI has a live website now</a></p>
                <p>June 2021: Pleased to be outstanding reviewer for CVPR 2021</p>
                <p>May 2021: During CVPR 2021, I'll be delivering keynotes during <a href="https://bryanyzhu.github.io/video-cvpr2021/">2nd comprehensive tutorial on Video understanding</a>, <a href="https://sites.google.com/view/fgvc8/program">8th workshop on FGVC</a> and <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">2nd HVU workshop</a>, in addition to co-organising <a href="https://eyewear-computing.org/EPIC_CVPR21/">8th EPIC@CVPR2021</a></p>
                <p>Mar 2021: CVPR 2021 paper "On Semantic Similarity in Video Retrieval" <a href="https://arxiv.org/abs/2103.10095">now on ArXiv</a></p>
                <p>Mar 2021: CVPR 2021 paper "Temporal-Relational CrossTransformers for Few-Shot Action Recognition" <a href="https://arxiv.org/abs/2101.06184">now on ArXiv</a></p>


                        <p>Feb 2021: Will be delivering keynotes at two CVPR2021 workshops: <a href="https://sites.google.com/view/fgvc8/program">Fine-grained visual categorisation (FGVC8)</a> and <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">Holistic Video Understanding (HVU)</a></p>
                <p>Jan 2021: Slides for my talk at <a href="http://staff.ustc.edu.cn/~tzzhang/dl-hau2020/program.html">HAU@ICPR 2021 workshop</a> can be found <a href="https://dimadamen.github.io/pdfs/Talk-ICPR-2021.pdf">here</a></p>
                <p>Nov 2020: Two papers presented at ACCV 2020: <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Price_Play_Fair_Frame_Contributions_in_Video_Models_ACCV_2020_paper.html">Play Fair: Frame Attributions in Video Models</a> <a href="http://play-fair.willprice.dev/">(Project)</a> and <br/><a href="https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html">Meta-Learning with Context-Agnostic Initialisations</a> <a href="https://tobyperrett.github.io/contextagnosticweb/">(Project)</a></p>
                <p>Nov 2020: Excited to start active work as <a href="http://iccv2021.thecvf.com/organizers">Program Chair for ICCV 2021</a> in Montreal</p>
                <p>Sep 2020: Glad to join the <a href="https://ellis.eu/members">ELLIS (European Laboratory for Learning and Intelligent Systems) Society</a> as a member</p>
                <p>Sep 2020: Will be giving a keynote at the <a href="https://sites.google.com/view/dvu2020-workshop/home/invited-speakers">Deep Video Understanding workshop alongside ICMI 2020</a></p>
                <p>Sep 2020: Will be giving a keynote at the <a href="https://hcma2020.github.io">HCMA workshop alongside ACM MM</a> on 12 Oct</p>
                <p>Aug 2020: Final program for our EPIC@ECCV Workshop is now <a href="https://eyewear-computing.org/EPIC_ECCV20/program">available online</a></p>
                <p>1 July 2020: EPIC-KTICHESN-100 Dataset Now Released - <a href="https://t.co/jKeXZQqh90?amp=1">Download</a>, <a href="https://t.co/lnz9JI486s?amp=1">webinar</a></p>
                <p>June 2020: <a href="https://arxiv.org/abs/2006.13256">ArXiv Manuscript "Rescaling Egocentric Vision"</a> is now online</p>
                <p>June 2020: Will be happy to deliver keynotes in August at ECCV workshops: <a href="https://sites.google.com/view/wicvworkshop-eccv2020/program/talks">Women in Computer Vision</a> and <a href="https://camp-workshop.stanford.edu">Compositional and Multimodal Video Perception Challenge</a></p>
                <p>Jun 2020: Proud to be an <a href="http://cvpr2020.thecvf.com/reviewer-acknowledgements#outstanding-reviewers">Outstanding Reviewer for CVPR 2020</a>.</p>
                <p>Jun 2020: My talk on "Learning from Narrated Videos of Everyday Tasks" at the CVPR2020 workshop on Instructional Videos is now available <a href="https://drive.google.com/file/d/1nMr6wanv9fQFjbJNP9ZjDQBMNVq8kUIT/view">online</a></p>
                <p>Jun 2020: Deadline for the 7th <a href="https://eyewear-computing.org/EPIC_ECCV20/">EPIC@ECCV2020 Workshop</a> is 10th of July - published proceedings included</p>
                <p>Apr 2020: <a href="https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/T028572/1">EPSRC Program Grant Visual AI</a> approved for funding. </p>

                
                
                        <p>Mar 2020: I join the <a href="https://www.springer.com/journal/11263/editors">editorial board of IJCV</a> as an associate editor.
                </p>
                <p>Mar 2020: Our paper "Action Modifers: Learning from Adverbs in Instructional Videos", accepted at CVPR 2020 is now on <a href="https://arxiv.org/abs/1912.06617">ArXiv</a> - <a href="https://youtu.be/rajo0x7WF-c">watch the video that explains the method here</a></p>
                <p>Mar 2020: Our paper "Multi-Modal Domain Adaptation for Fine-Grained Action Recognition", accepted for Oral at CVPR 2020 is now on <a href="https://arxiv.org/abs/2001.09691">Arxiv</a> - see <a href="https://jonmun.github.io/mmsada/">project details</a></p>
                <p>Feb 2020: Two papers accepted in CVPR 2020 - available in Arxiv already (see publications)</p>
                        <p>Dec 2019: <a href="https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/T022205/1">Jade-2 HPC Cluster</a> has been accepted for funding</p>
                <p>Dec 2019: My talk at NCVPRIPG2019 in Hubli India is available <a href="./Talks/NCVPRIPG2019-Talk-DimaDamen.pdf">here</a></p>
                <p>Dec 2019: I start EPSRC Early Career Fellowship <a href="https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/T004991/1">UMPIRE</a> this Jan. 5-Years Funding to expand my research activities in object interaction understanding.</p>
                        <p>Nov 2019: I join the editorial board of IEEE TPAMI as associate editor</p>
                <p>Nov 2019: Congrats to Davide Moltisanti and Michael Wray for passing their PhD vivas</p>
                <p>Oct 2019: My talk from the Extreme Vision ICCV workshop is available <a href="./Talks/ICCV2019-ScalingEgocentricVision-27Oct.pdf">here</a></p>
                <p>Oct 2019: Videos of all talks in our BMVA symposium are <a href="https://www.youtube.com/watch?v=uMZ_avMVUgM&list=PLW8VWHVjepItdBf7nDvs5MVMRZrF_l3jO">on available on YouTube</a></p>
                <p>Oct 2019: Videos of my 4-hour tutorial at North African Summer School in Machine Learning are now on YouTube: <a href="https://www.youtube.com/watch?v=_yGohdBVm5g">Part1</a>, <a href="https://www.youtube.com/watch?v=LrtVJiJmRis">Part2</a></p>
                <p>Sep 2019: Our paper "Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos" (ICCVW) is now on Arxiv - <a href="https://video-reversal.willprice.dev">Details</a></p>
                <p>Aug 2019: We release <a href="https://github.com/epic-kitchens/action-models">pretrained models</a> and <a href="https://arxiv.org/abs/1908.00867">technical report</a> for action recognition on EPIC-Kitchens</p>
    <p>July 2019: Our paper "Learning Visual Actions Using Multiple Verb-Only Labels" accepted for presentation at BMVC2019 is <a href="https://arxiv.org/abs/1907.11117">available on Arxiv</a></p>
      
      <p>July 2019: Two papers accepted at ICCV 2019, "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition" with Evangelos Kazakos (Bristol), Arsha Nagrani and Andrew Zisserman (Oxford), and "Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings" with Mike Wray (Bristol), Gabriela Csurka and Diane Larlus (Naver Labs)</p>
      <p>July 2019: I'll be giving keynotes at the <a href="https://research.google.com/youtube8m/workshop2019/">ICCV2019 workshop YouTube8M</a>, <a href="http://iplab.dmi.unict.it/acvr2019/">ICCV2019 workshop ACVR</a>, and at the <a href="https://egoappworkshop.wordpress.com">BMVC2019 workshop EgoApp</a></p>
      <p>July 2019: CFP <a href="./bmva_symposium_2019/">BMVA symposium on Video Understanding</a> in London - 25 Sep</p>
      <p>June 2018: Slides from my tutorial at <a href="https://nassma.um6p.ma">North African Summer School in Machine Learning (NASSMA)</a> are now <a href="./Talks/ML-Saves-CV-handout.pdf">online</a></p>


              <p>Apr 2019: <a href="https://tobyperrett.github.io/ddlstmweb/index.html">Camera ready version and details of our paper Dual-Domain LSTM for Cross-Dataset Action Recognition</a> - CVPR 2019 now online</p>
      <p>Apr 2019: <a href="./TheProsandCons/index.html">Camera ready version and details of our paper The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos</a> - CVPR 2019 now online</p>
      <p>Apr 2019: Fifth EPIC workshop accepted - <a href="http://eyewear-computing.org/EPIC_ICCV19/">EPIC@ICCV2019</a> will be held in Seoul this Oct/Nov</p>
      <p>Apr 2019: Will be giving a keynote at the <a href="http://iplab.dmi.unict.it/acvr2019/">7th Workshop on Assistive Computer Vision and Robotics</a> alongside ICCV in Seoul this October.</p>
      <p>Apr 2019: <a  href="single_timestamps/index.html">Camera ready version and details of our paper Action Recognition from Single Timestamp Supervision in Untrimmed Videos</a> - CVPR 2019 now online and <a href="https://arxiv.org/abs/1904.04689">on Arxiv</a></p>
      <p>Mar 2019: Looking forward to returning as a speaker for the <a href="http://cvss.blogs.lincoln.ac.uk">2019 BMVA summer school</a> in Lincoln this June</p>
      <p>Mar 2019: Slides for my keynote at VISAPP now <a href="Talks/VISAPP2019-small.pdf">online</a></p>
              <p>Mar 2019: Invited as one of the speakers at the First North African summer school in Machine Learning <a href="https://nassma.um6p.ma">NASSMA</a></p>
      <p>Feb 2019: <a href="./publications.html">Three papers accepted at CVPR 2019</a> (coming soon on Arxiv)</p>
      <p>Feb 2019: Keynote at <a href="http://www.visapp.visigrapp.org">VISAPP 2019 in Prague</a></p>
      <p>Dec 2018: <a href="http://eyewear-computing.org/">EPIC@CVPR2019</a> Workshop to take place in Long Beach</p>
              <p>Oct 2018: Scheduled talk at MPI Tubingen - <a href="https://ps.is.tuebingen.mpg.de/events/a-fine-grained-perspective-onto-object-interactions">details</a></p>
      <p>Sep 2018: EPIC-KITCHENS presented as Oral at ECCV 2018 - camera ready available <a href="https://arxiv.org/abs/1804.02748">Arxiv</a>, <a href="http://epic-kitchens.github.io/">Webpage</a></p>
      <p>Sep 2018: Leaderboards for the EPIC-KITCHENS challenges on CodaLab are now <a href="https://epic-kitchens.github.io/2018#challenges">open</a></p>
      <p>Sep 2018: Two papers presented at BMVC 2018. <a href="https://arxiv.org/abs/1805.06749">Action Completion</a> (<a href="https://github.com/FarnooshHeidari/CompletionDetection">Dataset</a>) and <a href="https://arxiv.org/abs/1806.08152">CaloriNet</a></p>
      <p>Aug 2018: EPIC-SKILLS dataset, for our CVPR2018 paper is now <a href="./Skill/index.html#dataset"></a>online</p>
      <p>Aug 2018: Programme for <a href="http://www.eyewear-computing.org">EPIC@ECCV2018 workshop</a> is now available. Join us in Munich on Sep 9th</p>
      <p>July 2018: Looking forward to giving a tutorial on <b>egocentric vision</b> as part of the <a href="https://cvss-uea.uk">BMVA Summer School</a> on July 5th, University of East Anglia.</p>
              <p>Apr 2018: EPIC-KITCHENS 2018 goes live today: 11.5M Frames, full HD, 60fps, head-mounted, 32 kitchens from 4 cities (in North America and Europe), 10 nationalities. FULLY annotated: 40K action segments, 454K object bounding boxes. Dataset: <a href="http://epic-kitchens.github.io">http://epic-kitchens.github.io</a> Details at: <a href="http://arxiv.org/abs/1804.02748">Arxiv</a></p>
      <p>Apr 2018: Our paper "Human Routine Change Detection using Bayesian Modelling" accepted at ICPR2018, to be presented in Beijing this August. <a href="./Routine/">Details</a></p>
      <p>Feb 2018: Our paper "Who's Better, Who's Best" accepted at CVPR2018, to be presented in Salt Lake city this June. <a href="./Skill/">Details</a></p>
      <p>Feb 2018: Congrats to <a href="http://www.irc-sphere.ac.uk/uob-yangdi">Yangdi Xu</a> for passing his PhD viva with minor corrections</p>
              <p>Jan 2018: Serving on the High Performance Computing exec board at UoB</p>
      <p>Oct 2017: Selected as outstanding reviewer at ICCV 2017</p>
      <p>Oct 2017: Well-attended <a href="http://www.eyewear-computing.org/EPIC_ICCV17/">Second Int. Egocentric Perception Interaction and Computing (EPIC) workshop alongside ICCV in Venice</a></p>
      <p>Oct 2017: Our paper "Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video" was presented in ICCV - <a href="./Trespass/">details</a></p>
      <p>Oct 2017: Our paper "Recurrent Assistance: Cross-Dataset Training of LSTMs on Kitchen Tasks" was presented at ACVR - <a href="./LSTMT/acvr2017.pdf">pdf</a></p>
      <p>Sep 2017: <a href="https://comsm0018-applied-deep-learning.github.io/">Unit details for the <font color="red"><b>new</b></font> Applied Deep Learning M level unit</a> - <a href="http://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=17/18&unitCode=COMSM0018">Uni Catalogue</a>
      <p>Sep 2017: Serving as <a href="https://www.journals.elsevier.com/pattern-recognition/editorial-board">associate editor for Pattern Recognition</a></p>
      <p>Aug 2017: Happy to be working as a consultant with Bristol's Cookpad on developing their Computer Vision and Machine Learning agenda</p>
              <p>Apr 2017: <a href="http://cvss.blogs.lincoln.ac.uk/programme/">6 July - Lincoln, BMVA summer school, will be giving a tutorial on Egocentric Vision</a></p>
      <p>Apr 2017: <a href="https://pintofscience.co.uk/event/rage-against-the-machine-vision">15 May - Bristol, will be giving a public talk on 'What can a wearable camera know about me?'</a></p>
              <p>Mar 2017: <a href="./Talks/DamenBMVAMar2017RGBD.pdf">Slides from my talk on Challenges and Opportunities for Action and Activity Recognition using RGBD Data</a>, BMVA Symposium are now available</p>
      <p>Feb 2017: <a href="https://onedrive.live.com/?authkey=%21AA%5Fh1NXcVe4TSyo&id=7534AF006761B2BF%21440800&cid=7534AF006761B2BF">Videos from BMVA symposium on Transfer Learning now available</a></p>
      <p>Dec 2016: <a href="TLCV/">Final programme for BMVA Symposium on Transfer Learning now available</a></p> 
    
              <p>Oct 2016: <a href="https://github.com/BristolVisualPFT/3D_Data_Acquisition_Registration_Using_Kinects/tree/master/Double_opposing_Kinects">Code Released</a> for 3DV paper on acquisition and registration of point clouds using two facing Kinects </p>
      <p>Sep 2016: 3D Data Acquisition and Registration Using Two Opposing Kinects - Paper accepted at 3DV</p>
      <p>Aug 2016: Action Completion paper accepted at BMVC and dataset released - <a href="./ActionCompletion/">Project Webpage</a></p> 
        <p>Aug 2016: Nokia Technologies donation of &euro;50K <a href="http://www.bristol.ac.uk/news/2016/august/nokia-grant.html">Press release</a></p>
        <p>Aug 2016: Action Completion paper accepted at BMVC, York, Sep 2016. <a href="./ActionCompletion/">project</a></p>

              <p>July 2016: PhD students Michael Wray and Davide Moltisanti awarded second poster prize at BMVA summer school <a href="https://vilab.blogs.ilrt.org/?p=1779">news</a>
    <p>July 2016: CFP: Transfer Learning in Computer Vision - BMVA Symposium <a href="TLCV/">[details, dates and submission]</a></p>
    <p>Jun 2016: PhD opening in Computer Vision and Machine Learning (Home/EU students) - <a href="http://www.bris.ac.uk/engineering/media/grad-school/scholarships/locate.pdf">Open Until </a>, <a href="http://www.jobs.ac.uk/job/ANY901/phd-in-computer-vision-and-machine-learning-epsrc-project-locate/">ad on jobs.ac.uk</a></p>
    <p>Jun 2016: <a href="http://cvpr2016.thecvf.com/program/demos">CVPR 2016 Demo for GlaciAR</a></p>
    <p>May 2016: <a href="http://www.eyewear-computing.org/EPIC_ECCV16/">EPIC@ECCV2016 Workshop (Egocentric Perception, Interaction and Computing) Accepted.</a></p>
    <p>Apr 2016: <a href="http://gow.epsrc.ac.uk/NGBOViewPanelROL.aspx?PanelId=1-2L4MKR&RankingListId=1-2L4MKZ">EPSRC Project LOCATE</a> to be funded starting July 2016</p>
    
            <p>Mar 2016: <a href="http://arxiv.org/abs/1510.04862v2">You-Do, I-Learn: Egocentric Unsupervised Discovery of Objects and their Modes of Interaction Towards Video-Based Guidance</a> accepted at CVIU</p>
        <p>Nov 2015: <a href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N013964/1">EPSRC Project GLANCE</a> to be funded starting March 2016</p>
    <p>Sep 2015: Paper "Unsupervised Daily Routine Modelling from a Depth Sensor using Bottom-Up and Top-Down Hierarchies" accepted at ACPR</p>
    <p>Sep 2015: Paper "Estimating Visual Attention from a Head Mounted IMU" presented at ISWC in Okasa, Japan</p>
    <p>Sep 2015: Paper "Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling" Presented at BMVC</p>
    <p>Aug 2015: PhD Student Vahid Soleiman publishes paper on Remote Pulmonary Function Testing using a Depth Sensor at BIOCAS 2015.
      <a href="https://youtu.be/AX4BvyoKYYQ">Demo Video</a>
    <p>June 2015: PLOSONE article <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127769">available online</a></p>
    <p>June 2015: SI on cognitive robotics systems: concepts and applications at the Journal of Intelligent &amp; Robotic Systems is
      <a href="http://dx.doi.org/10.1007/s10846-015-0244-9">online</a></p>
    <p>Mar 2015: Interdisciplinary research internship <a href="http://www.cs.bris.ac.uk/news/news-item.jsp?nid=285">awarded for CS student Hazel Doughty</a>.</p>
      <p>Sep 2014: Paper <a href="You-Do-I-Learn">"You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video"
      </a> presented at BMVC</p>
    <p>Sep 2014: Paper <a href="Sphere/bmvc14.pdf">"Online quality assessment of human movement from skeleton data"
      </a> presented at BMVC</p>
    <p>Sep 2014: Paper <a href="You-Do-I-Learn">"Multi-user egocentric Online System for Unsupervised Assistance on Object Usage"
      </a> presented at ECCVW (ACVR 2014)</p>
    <p>Aug 2014: C++ code (improved performance v 1.2) and Android apk <a href="MultiObjDetector.htm">for real-time object detector</a> available.</p>
    <p>July 2014: The <a href="BEOID">Bristol Egocentric Object Interactions Dataset</a> has now been released</p>
    <p>July 2014: <a href="newsletter-2014-03-BookReview.pdf">Book review published in IAPR newsletter</a></p>
      <p>July 2014: Video lectures from BMVC 2013 <a href="http://videolectures.net/bmvc2013_bristol/">are now available online</a>.</p>
      <p>July 2014: Sphere project's website <a href="http://irc-sphere.ac.uk">irc-sphere.ac.uk</a> has now been released (2013-2018).</p>
      <p>May 2014: <a href="projects.htm">Project Ideas for 3G403 and 2G400 students available</a>.</p>
      <p>Feb 2014: <a href="jobs.htm">PhD Opening in Ego-centric Vision</a> - apply as soon as possible... </p>
      <p>Nov 2013: Project GlaciAR is starting funded by <a href="http://www.sait.samsung.co.kr/saithome/Page.do?method=main&pagePath=01_about/&pageName=2013gro">Samsung's GRO 2013 Awards</a></p>
      <p>Nov 2013: <a href="http://crs2013.org">Cognitive Robotics Systems (CRS 2013)</a> (<a href="http://www.iros2013.org/">IROS 2013</a> workshop), successfully concludes at Tokyo, Japan.</p>
      <p>Sep 2013: <a href="http://bmvc2013.bristol.ac.uk">BMVC 2013</a> successfully concluded at Bristol</p>
      <p>Aug 2013: Outstanding Reviewer award at <a href="http://www.avss2013.org/awards">IEEE AVSS 2013</a>.</p>
      <p>June 2013: Outstanding Reviewer award at <a href="http://www.pamitc.org/cvpr13/">IEEE CVPR 2013</a>.</p>
      <p>May 2013: New bug-fixed version (v1.1) of our Multi-Object Detector (MOD) code is now available <a href="MultiObjDetector.htm">online</a></p>
    </div>

          </td><td valign="top">
      <a class="twitter-timeline" data-width="400" data-height="750" href="https://twitter.com/dimadamen">Tweets by dimadamen</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</td></tr></table>

    <a name="Projects"><h2>Research Projects</h2></a>
      
    <h3><a href="https://ekazakos.github.io/MTCN-project/">Temporal Context in Egocentric Video</a></h3>
    <a href="https://ekazakos.github.io/MTCN-project/"><img src="./MTCN.png"/></a>
    
    <a href="https://youtu.be/AkvyX79RVvw">Video</a><br/>
    <p>With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition. E Kazakos, J Huh, A Nagrani, A Zisserman, D Damen. BMVC (2021). <a href="https://arxiv.org/abs/2111.01024">ArXiv Paper</a>
     | <a href="https://ekazakos.github.io/MTCN-project/">Project Webpage</a> | Code (Coming Soon)</p>
      
      <h3><a href="https://ego4d-data.org/">Ego4D: Around the World in 3,000 Hours of Egocentric Video</a></h3>
    <a href="https://arxiv.org/abs/2110.12812"><img src="./Ego4D.png"/></a>
    
      <a href="https://ego4d-data.org/">Ego4D Project</a> | <a href="https://www.youtube.com/watch?v=2dau0W0NVQY">Reveal Session Video</a> | <a href="https://drive.google.com/file/d/1oknfQIH9w1rXy6I1j5eUE6Cqh96UwZ4L/view?usp=sharing">Trailer Video</a><br/>
      
    <p>Around the World in 3,000 Hours of Egocentric Video. K Grauman (+83 Authors) et al. ArXiv (2021). <a href="https://arxiv.org/abs/2110.07058">ArXiv</a>
    </p>

    <h3><a href="http://epic-kitchens.github.io">Rescaling Egocentric Vision</a></h3>
          <table><tr><td width="40%">  <a href="http://epic-kitchens.github.io"><video autoplay muted loop width="100%">
          <source src="https://epic-kitchens.github.io/static/videos/03x03_videoWall.webm" type="video/webm">
          <source src="https://epic-kitchens.github.io/static/videos/03x03_videoWall.mp4" type="video/mp4">
          <source src="https://epic-kitchens.github.io/static/videos/03x03_videoWall.mp4" type="video/mp4">
          Sorry, we cannot display the EPIC-Kitchens 2018 video wall as
          your browser doesn't support HTML5 video.
              </video></a></td><td valign="top">
        <p><a href="https://youtu.be/8IzkrWAfAGg">Trailer</a> | <a href="https://youtu.be/MUlyXDDzbZU">Video Demonstration</a> | <a href="https://t.co/lnz9JI486s?amp=1">Webinar</a> | <a href="https://t.co/jKeXZQqh90?amp=1">Download</a></p>
    <p>Rescaling Egocentric Vision. D Damen, H Doughty, G Farinella, A Furnari, E Kazakos, J Ma, D Moltisanti, J Munro, T Perrett, W Price, M Wray. IJCV. <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01531-2.pdf">IJCV paper</a>, <a href="https://arxiv.org/abs/2006.13256">ArXiv</a>, <a href="http://epic-kitchens.github.io">Webpage</a>
    </p>
              <p>The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines. D Damen, H Doughty, GM Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. IEEE Transactions on Pattern Analysis and Machine Intelligence 43(11) pp 4125-4141 (2021). <a href="https://ieeexplore.ieee.org/document/9084270">IEEE</a>, <a href="https://arxiv.org/abs/2005.00343">Arxiv Preprint</a></p>
              </td></tr></table>
      
    <h3><a href="https://arxiv.org/abs/2110.12812">Domain Adaptation in Video Retrieval</a></h3>
    <a href="https://arxiv.org/abs/2110.12812"><img src="./Jonny2021.png"/></a>
    
    <p>Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval. J Munro, M Wray, D Larlus, G Csurka, D Damen. ArXiv (2021). <a href="https://arxiv.org/abs/2110.12812">ArXiv Paper</a>
    </p>  

    <h3><a href="https://mwray.github.io/SSVR/">Semantic Similarity in Video Retrieval</a></h3>
    <a href="https://mwray.github.io/SSVR/"><img src="./SSR.png"/></a>
    
    <p>On Semantic Similarity in Video Retrieval. M Wray, H Doughty, D Damen. CVPR (2021). <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wray_On_Semantic_Similarity_in_Video_Retrieval_CVPR_2021_paper.pdf">CVF PDF</a> | <a href="https://arxiv.org/abs/2103.10095">ArXiv Preprint</a> | <a href="https://mwray.github.io/SSVR/">Project Webpage</a> | <a href="https://youtu.be/pS9qa_B771I">Video</a>
    </p>    

    <h3><a href="https://tobyperrett.github.io/trxweb/">Temporal-Relational CrossTransformers</a></h3>
    <a href="https://tobyperrett.github.io/trxweb/"><img src="./TRX.png"/></a>
    
    <p>Temporal-Relational CrossTransformers for Few-Shot Action Recognition. T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen. CVPR (2021). <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf">CVF PDF</a> | <a href="https://arxiv.org/abs/2101.06184">ArXiv Preprint</a> | <a href="https://github.com/tobyperrett/trx">Code and Model</a> | <a href="https://tobyperrett.github.io/trxweb/">Project Webpage</a>
    </p>    

    <h3><a href="https://ekazakos.github.io/auditoryslowfast/">Slow-Fast Auditory Streams</a></h3>
    <a href="https://ekazakos.github.io/auditoryslowfast/"><img src="./ICASSP2021.png"/></a>
    
    <p>Slow-Fast Auditory Streams For Audio Recognition. E Kazakos, A Nagrani, A Zisserman, D Damen. ICASSP (2021). <a href="https://arxiv.org/abs/2103.03516">ArXiv Preprint</a> | <a href="https://ieeexplore.ieee.org/document/9413376">IEEE PDF</a> | <a href="https://github.com/ekazakos/auditory-slow-fast">Code and Models</a> | <a href="https://ekazakos.github.io/auditoryslowfast/">Project Webpage</a> <b style="color:red;">[Outstanding Paper]</b>
    </p>    

    <h3><a href="https://play-fair.willprice.dev">Frame Attributions in Video Models</a></h3>
    <a href="https://play-fair.willprice.dev"><img src="./PlayFair.png"/></a>
    <p><a href="http://play-fair.uksouth.cloudapp.azure.com/?uid=137966&n-frames=10" target="_blank">Interactive Dashboard</a> | <a href="https://youtu.be/hBkT_5C7LMQ">Teaser Video </a> | <a href="https://github.com/willprice/play-fair/">Code</a></p>
    <p>Play Fair: Frame Attributions in Video Models. W Price, D Damen. ACCV (2020). <a href="https://arxiv.org/abs/2011.12372">ArXiv Preprint</a> | <a href="https://play-fair.willprice.dev">Project Details</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Price_Play_Fair_Frame_Contributions_in_Video_Models_ACCV_2020_paper.html">CVF</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Price_Play_Fair_Frame_Contributions_in_Video_Models_ACCV_2020_paper.pdf">CVF PDF</a>
    </p>

    <h3><a href="https://tobyperrett.github.io/contextagnosticweb/">MetaLearning with Context-Agnostic Initialisation</a></h3>
<a href="https://tobyperrett.github.io/contextagnosticweb/"><img src="./ContextAgnostic.png"/></a>
    <p><a href="https://youtu.be/SrksZ-motho">Talk Video</a></p>
    <p>MetaLearning with Context-Agnostic Initialisation. T Perrett, A Masullo, T Burghard, M Mirmehdi, D Damen. ACCV (2020). <a href="https://arxiv.org/abs/2007.14658">ArXiv Preprint</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html">CVF</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.pdf">CVF PDF </a> | <a href="https://tobyperrett.github.io/contextagnosticweb/">Project Details</a>
    </p>




    <h3><a href="https://hazeldoughty.github.io/Papers/ActionModifiers/">Action Modifiers: Learning from Adverbs in Instructional Videos</a></h3>
    <a href="https://hazeldoughty.github.io/Papers/ActionModifiers/"><img src="./Adverbs-concept.png"/></a>
<p><a href="https://youtu.be/rajo0x7WF-c" target="_blank">Video</a>, <a href="https://hazeldoughty.github.io/Papers/ActionModifiers/talk.mp4">Talk Video </a></p>
    <p>Action Modifiers: Learning from Adverbs in Instructional Videos. H Doughty, I Laptev, W Mayol-Cuevas, D Damen. CVPR (2020). <a href="https://arxiv.org/abs/1912.06617">ArXiv Preprint</a>, <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Doughty_Action_Modifiers_Learning_From_Adverbs_in_Instructional_Videos_CVPR_2020_paper.pdf">CVF PDF</a>, <a href="https://hazeldoughty.github.io/Papers/ActionModifiers/">Project Details</a>
    </p>

    <h3><a href="https://jonmun.github.io/mmsada/">Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</a></h3>
    <a href="https://jonmun.github.io/mmsada/"><img src="./MMSADA.png"/></a>
<p><a href="https://youtu.be/qgd-DBgf-S0" target="_blank">Video</a>, <a href="https://www.youtube.com/watch?v=ldbPbXYQ5Js&feature=emb_logo">Oral Presentation Video</a></p>
    <p>Multi-modal Domain Adaptation for Fine-grained Action Recognition. J Munro, Dima Damen. CVPR (2020). <a href="https://arxiv.org/abs/2001.09691">ArXiv Preprint</a>, <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.pdf">CVF PDF</a>, <a href="https://jonmun.github.io/mmsada/">Project Details</a>, <a href="https://github.com/jonmun/MM-SADA-code">Code</a>
    </p>

    <h3><a href="https://video-reversal.willprice.dev">Retro-Actions</a></h3>
    <a href="https://video-reversal.willprice.dev"><img src="./Retro.png"/></a>
<p><a href="https://www.youtube.com/watch?v=yuhucAhy8yI" target="_blank">Video</a></p>
    <p>Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos. W Price, Dima Damen. ICCV (2019). <a href="https://arxiv.org/abs/1909.09422">ArXiv Preprint</a>, <a href="https://video-reversal.willprice.dev">Project Details</a>
    </p>

    <h3><a href="https://mwray.github.io/FGAR/">Fine-Grained Action Retrieval</a></h3>
    <a href="https://mwray.github.io/FGAR/"><img src="./Fine-grained/main_fig.png"/></a>
<p><a href="https://youtu.be/FLSlRQBFow0" target="_blank">Video</a></p>
    <p>Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings. Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen. ICCV (2019). <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf">CVF PDF</a>, <a href="https://arxiv.org/abs/1908.03477">ArXiv Preprint</a>, <a href="https://mwray.github.io/FGAR/">Project Details</a>
    </p>

    <h3><a href="https://ekazakos.github.io/TBN/">Audio-Visual Temporal Binding for Egocentric Action Recognition</a></h3>
    <a href="https://ekazakos.github.io/TBN/"><img src="./TBN/mainFig.png"/></a>
<p><a href="https://www.youtube.com/watch?v=nhUoCbJ3_IQ&feature=youtu.be">Talk Video</a>,  <a href="https://youtu.be/VzoaKsDvv1o" target="_blank">Video</a></p>
    <p>EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition. Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen. ICCV (2019). <a href="https://ekazakos.github.io/TBN/">Project Details</a>, <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf">CVF PDF</a>, <a href="https://arxiv.org/abs/1908.08498">Arxiv Preprint</a>
    </p>

    <h3><a href="https://mwray.github.io/MVOL/">Learning Visual Actions Using Multiple Verb-Only Labels</a></h3>
    <a href="https://mwray.github.io/MVOL/"><img src="./Unequivocal/dilemma.png" /></a>
<p><a href="https://www.youtube.com/watch?v=GEJRi5etiaE" target="_blank">Video</a></p>
    <p>Learning Visual Actions Using Multiple Verb-Only Labels. M Wray, D Damen. BMVC (2019). <a href="https://arxiv.org/abs/1907.11117">ArXiv Preprint</a>, <a href="https://mwray.github.io/MVOL/">Project Details</a>
    </p>

    <h3><a href="https://tobyperrett.github.io/ddlstmweb/index.html">DDLSTM: Dual-Domain LSTM</a></h3>
    <a href="https://tobyperrett.github.io/ddlstmweb/index.html"><img src="intro_DDLSTM.jpg"/></a>
   <p><a href="https://youtu.be/8MtC6X4w4jE" target="_blank">Video</a></p> 
    <p> DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition. T Perrett and D Damen. CVPR (2019). <a href="https://tobyperrett.github.io/ddlstmweb/main.pdf">pdf preprint</a>, <a href="https://arxiv.org/abs/1904.08634">Arxiv</a> <a href="https://tobyperrett.github.io/ddlstmweb/index.html">Project Details</a></p>

    <h3><a href="./TheProsandCons/index.html">The Pros and Cons: Rank-Aware Attention Modules</a></h3>
    <a href="./TheProsandCons/index.html"><img src="./TheProsandCons/concept.png"/></a>
   <p><a href="./TheProsandCons/teaser.mp4">Teaser Video</a></p>
   <p><a href="https://youtu.be/ILvowKqiALU" target="_blank">Results Video</a></p> 
    <p> The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos. H Doughty, W Mayol-Cuevas, D Damen. CVPR (2019). <a href="./TheProsandCons/TheProsandCons.pdf">pdf preprint</a>, <a href="https://arxiv.org/abs/1812.05538">Arxiv</a>, <a href="./TheProsandCons/index.html">Project Details</a></p>
    
    <h3><a href="./single_timestamps/index.html">Action Recognition from Single Timestamps</a></h3>
    <a href="./single_timestamps/index.html"><img src="./single_timestamps/intro2.png"/></a>
   <p><a href="https://youtu.be/K_R3MPvcJhc">Results Video</a></p> 
    <p> Action Recognition from Single Timestamp Supervision in Untrimmed Videos. D Moltisant, S Fidler and D Damen. CVPR (2019). <a href="single_timestamps/paper/Action_Recognition_with_Single_Timestamp_Supervision__CVPR_.pdf">pdf preprint</a>, <a href="single_timestamps/index.html">Project Details</a></p>

    <h3><a href="https://sites.google.com/view/epic-tent">Tent Assembly Egocentric Dataset</a></h3>
    <a href="https://sites.google.com/view/epic-tent"><img src="./epic-tent.png"/></a>
   <p><a href="https://youtu.be/MGqT6J6JJ4I">Video</a></p> 
    <p>(2021) B Sullivan, C Ludwig, <u>D Damen</u>, W Mayol-Cuevas, I Gilchrist. Look-Ahead Fixations During Visuomotor Behavior: Evidence from Assembling a Camping Tent. Journal of Vision 21(3):13. <a href="https://jov.arvojournals.org/article.aspx?articleid=2772371">PDF</a></p>
    <p> EPIC-Tent: An Egocentric Video Dataset for Camping Tent Assembly. Y Jang, B Sullivan, C Ludwig, I.D. Gilchrist, D Damen and W Mayol-Cuevas. ICCV Workshops (2019). <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Jang_EPIC-Tent_An_Egocentric_Video_Dataset_for_Camping_Tent_Assembly_ICCVW_2019_paper.pdf">pdf</a>, <a href="https://sites.google.com/view/epic-tent">Project Details</a>, <a href="https://www.google.com/url?q=https%3A%2F%2Fdata.bris.ac.uk%2Fdata%2Fdataset%2F2ite3tu1u53n42hjfh3886sa86&sa=D&sntz=1&usg=AFQjCNFqid3XlHOLRR1zaw4gSHFTq_VmwA">Dataset</a>, <a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fyoungkyoonjang%2FEPIC_Tent2019&sa=D&sntz=1&usg=AFQjCNF_rH_7LMUSGmkuDi8pzEboQgEALQ">Annotations</a></p>
    

    <h3><a href="http://epic-kitchens.github.io">Scaling Egocentric Vision: EPIC-KITCHENS 2018</a></h3>
          <table><tr><td width="40%">  <a href="http://epic-kitchens.github.io"><video autoplay muted loop width="100%">
          <source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">
          <source src="https://epic-kitchens.github.io/static/videos/04x04_h265_720.mp4" type="video/mp4">
          <source src="https://epic-kitchens.github.io/static/videos/04x04_h264_720.mp4" type="video/mp4">
          Sorry, we cannot display the EPIC-Kitchens 2018 video wall as
          your browser doesn't support HTML5 video.
              </video></a></td><td valign="top">
        <p><a href="https://youtu.be/Dj6Y3H0ubDw">Video</a></p>
    <p>Scaling Egocentric Vision: The EPIC-KITCHENS Dataset. D Damen, H Doughty, G Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. ECCV (2018). <a href="http://epic-kitchens.github.io">Webpage</a> | <a href="http://epic-kitchens.github.io">Dataset</a> | <a href="https://arxiv.org/abs/1804.02748">arxiv</a>
    </p>
              <p>An Evaluation of Action Recognition Models on EPIC-Kitchens. W Price, D Damen. Arxiv (2019)  <a href="https://arxiv.org/abs/1908.00867">Arxiv</a> | <a href="https://github.com/epic-kitchens/action-models">Github</a> | <a href="EPICModels/price_epic_models2019.pdf">PDF</a></p>
              
              <p>The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines. D Damen, H Doughty, GM Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020). <a href="https://arxiv.org/abs/2005.00343">Arxiv Preprint</a></p>
              </td></tr></table>
    
    <h3><a href="./Skill/">Skill Determination in Video</a></h3>
    <a href="./Skill/"><img src="./Skill/concept.png" /></a>
   <p><a href="https://youtu.be/GO5pBQA5PhI">Video</a></p> 
    <p>Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination. H Doughty, D Damen, W Mayol-Cuevas. CVPR (2018). <a href="Skill/whos_better_whos_best.pdf">PDF</a> | <a href="https://arxiv.org/abs/1703.09913">arxiv</a> | <a href="https://github.com/hazeld/EPIC-Skills2018">Dataset</a>
    </p>
    

    <h3><a href="./ActionCompletion/">Action Completion: A Temporal Model for Moment Detection</a></h3>
    <a href="ActionCompletion/"><img src="ActionCompletion/ActionCompletionDetectionIntro.png" /></a>
<p>Weakly-Supervised Completion Moment Detection using Temporal Attention. F Heidarivincheh, M Mirmehdi, D Damen. ICCV Workshop on Human Behaviour Understanding. <a href="https://arxiv.org/abs/1910.09920">Arxiv</a> | <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/HBU/Heidarivincheh_Weakly-Supervised_Completion_Moment_Detection_using_Temporal_Attention_ICCVW_2019_paper.pdf">CVF PDF</a>, Oct 2019.</p>
   <p><a href="https://youtu.be/Hrxehk3Sutc">Video2018</a>, <a href="https://youtu.be/iBdW-kVKMds">Video2016</a></p> 
    <p>Action Completion: A Temporal Model for Moment Detection. F Heidarivincheh, M Mirmehdi, D Damen. British Machine Vision Conference (BMVC), Sep 2018. <a href="https://arxiv.org/abs/1805.06749">Arxiv PDF</a> | <a href="https://github.com/FarnooshHeidari/CompletionDetection">Dataset</a> </p>
    <p>Beyond Action Recognition: Action Completion in RGB-D Data. F Heidarivincheh, M Mirmehdi, D Damen. British Machine Vision Conference (BMVC), Sep 2016.
     <a href="./ActionCompletion/ActionCompletion_BMVC2016.pdf">pdf</a> | <a href="./ActionCompletion/ActionCompletion_BMVC2016_abstract.pdf">abstract</a> |
      <a href="http://dx.doi.org/10.5523/bris.66qry08cv1fj1eunwxwob3fjz">Dataset</a>
    </p>
    
    <h3><a href="./Routine/">Human Routine Modelling and Change Detection</a></h3>
    <a href="./Routine/"><img src="./Routine/kitchenDataset.png" /></a>
    <p>Human Routine Change Detection using Bayesian Modelling. Y Xu, D Damen. ICPR (2018). <a href="Routine/ICPR18_XuDamen.pdf">PDF</a>
    </p>
    <p>Unsupervised Long-Term Routine Modelling using Dynamic Bayesian Networks. Y Xu, D Bull, D Damen. DICTA (2017). <a href="Routine/DICTA17_XuBullDamen.pdf">PDF</a></p>

    
    <h3><a href="./Trespass/">Trespassing the Boundaries of Object Interactions</a></h3>
    <a href="./Trespass/"><img src="./Trespass/Overview.png" /></a>
   <p><a href="https://youtu.be/mWr8JJDgA3w">Video</a></p> 
    <p>Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video. D Moltisanti, M Wray, W Mayol-Cuevas, D Damen. International Conference on Computer Vision (ICCV), 2017. <a href="./Trespass/trespassingBoundariesLabeling.pdf">pdf</a> (camera ready) | <a href="https://arxiv.org/abs/1703.09026">arxiv</a>
    </p>
    
    <h3><a href="./SEMBED/">Semantic Embedding for Egocentric Actions</a></h3>
    <a href="./SEMBED/"><img src="./SEMBED/Overview2.png" /></a>
   <p><a href="http://youtu.be/6bDDTIJUuic">Video</a></p> 
    <p>SEMBED: Semantic Embedding of Egocentric Action Videos. M Wray, D Moltisanti, W Mayol-Cuevas, D Damen. Egocentric Interaction, Perception and Computing (EPIC), European Conference on Computer Vision Workshops (ECCVW), Oct 2016.
     <a href="./SEMBED/ECCVW2016SEMBED.pdf">pdf</a> | <a href="BEOID">Dataset</a>
    </p>
      
    

    <h3><a href="You-Do-I-Learn">You-Do, I-Learn</a></h3>
    <img src="YDIL.png" />
    <p><a href="http://www.youtube.com/watch?v=vUeRJmwm7DA">Video1 (2014)</a>, <a href="https://www.youtube.com/watch?v=USFaDjXkPfs">Video2 (2017)</a></p>
    <p>Automated capture and delivery of assistive task guidance with an eyewear computer: The GlaciAR system. T Leelasawassuk, D Damen, W Mayol-Cuevas. Augmented Human, Mar 2017
    <a href="https://arxiv.org/abs/1701.02586">pdf</a></p>
    <p>You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video. D Damen, T Leelasawassuk, O Haines, A Calway, W Mayol-Cuevas. British Machine Vision Conference (BMVC), Sep 2014.
      <a href="You-Do-I-Learn/Damen_BMVC2014.pdf">PDF</a> |
      <a href="You-Do-I-Learn/Damen_BMVC2014_abstract.pdf">Abstract</a> |
      <a href="BEOID">Dataset</a>
    </p>
                
    <p>Multi-user egocentric Online System for Unsupervised Assistance on Object Usage. D Damen, O Haines, T Leelasawassuk, A Calway, W Mayol-Cuevas. ECCV Workshop on Assistive Computer Vision and Robotics (ACVR), Sep 2014.
      <a href="You-Do-I-Learn/Damen_ACVR2014_prePrint.pdf">PDF Preprint</a>
    </p>
    
    <p class="projectPublication">Estimating Visual Attention from a Head Mounted IMU. T Leelasawassuk, D Damen, W Mayol-Cuevas. International Symposium on Wearable Computers (ISWC), Sep 2015.
      <a href="http://dx.doi.org/10.1145/2802083.2808394">PDF</a>
    </p>

    
    <h3><a href="http://www.irc-sphere.ac.uk/work-package-2/DS-KCF">DS-KCF: Depth-Based Real-Time Single Object Tracker</a></h3>
    <img src="DSKCF.png" />
    <p>
      <a href="https://www.youtube.com/watch?v=yhT2PdN9BTw&amp;feature=youtu.be">Video 1</a> |
      <a href="https://www.youtube.com/watch?v=YoFMf2iARzA&amp;feature=youtu.be">Video 2</a> |
      <a href="http://data.bris.ac.uk/data/dataset/16vbnj3im1ygi1sh0yd0mt4lp0">Code</a>
    </p>

    <p>Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling. M Camplani, S Hannuna, M Mirmehdi, D Damen, L Tao, T Burghardt and A Paiment. British Machine Vision Conference (BMVC), Sep 2015. <a href="http://bmvc2015.swansea.ac.uk/proceedings/papers/paper145/paper145.pdf">PDF</a>.</p>


    <h3><a href="MultiObjDetector.htm">Real-time Learning and Detection of 3D Texture-minimal Objects</a></h3>
    <img src="multiObj.png" />
    <p><a href="https://www.youtube.com/watch?v=XGRzjAFO5Qs">Video</a> | <a href="MultiObjDetector_code.zip">Code</a></p>

    
    <p>Real-time Learning and Detection of 3D Texture-minimal Objects: A Scalable approach. D Damen, P Bunnun, A Calway, W Mayol-Cuevas. British Machine Vision Conference (BMVC), Sep 2012.      
      <a href="bmvc2012_scalable_textureless.pdf">PDF</a> |
      <a href="bmvc2012_abstract.pdf">Abstract</a> |
      <a href="MultiObjDetector_code.zip">Code</a> |
      <a href="https://www.youtube.com/watch?v=4rPjN1mcKGc">Video</a> |
      <a href="http://www.cs.bris.ac.uk/Publications/pub_master.jsp?id=2001575">Dataset</a>.
    </p>
    
    <p>Efficient Texture-less Object Detection for Augmented Reality Guidance. T Hodan, D Damen, W Mayol-Cuevas, J Matas. IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR) Workshop on Visual Recognition and Retrieval for Mixed and Augmented Reality, Sep 2015.</p>
                

    <h3><a href="http://www.ict-cognito.org">Egocentric Real-time Industrial Workflow</a></h3>
    <img src="cognito.png" />
    <p><a href="https://youtu.be/wqDNxhAKkNI">Video 1</a> | <a href="https://www.youtube.com/watch?v=XGRzjAFO5Qs">Video 2</a></p>
    
    <p>Cognitive Learning, Monitoring and Assistance of Industrial Workflows Using Egocentric Sensor Networks. G Bleser, D Damen, A Behera, et al. PLOSONE, June 2015
      <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127769">PDF</a>.</p>
                
    <p>Egocentric Real-time Workspace Monitoring using an RGB-D Camera. D Damen, A Gee, W Mayol-Cuevas, A Calway. Intelligent Robotics and Systems (IROS), Oct 2012.
      <a href="Egocentric_IROS2012.pdf">PDF</a> | <a href="https://www.youtube.com/watch?v=XGRzjAFO5Qs">Video</a>.
    </p>

    <h3><a href="http://www.irc-sphere.ac.uk/work-package-2/movement-quality">Online Quality Assessment for Human Motion</a></h3>
    <img src="Quality.png" />
    <p><a href="http://www.cs.bris.ac.uk/home/palement/datasets/code_movement_quality.zip">Code</a></p>

    <p>Online quality assessment of human movement from skeleton data. A Paiment, L Tao, S Hannuna, M Camplani, D Damen and M Mirmehdi. British Machine Vision Conference (BMVC), Sep 2014.
      <a href="http://www.cs.bris.ac.uk/home/palement/articles/bmvc2014.pdf">PDF</a> | <a href="http://data.bris.ac.uk/data/dataset/bgresiy3olk41nilo7k6xpkqf">Dataset</a>.</p>

    <h3><a href="./Bicycles/bikes2.htm">The Bicycle Problem</a></h3>
    <img src="bikes2.jpg" />
    <p>Explaining Activities as Consistent Groups of Events - A Bayesian Framework using Attribute Multiset Grammars. D Damen and D Hogg International Journal of Computer Vision (IJCV), 2012. <a href="http://www.springerlink.com/content/x4vm7237u646u523/">PDF</a>.</p>
    <p>Recognizing Linked Events: Searching the Space of Feasible Explanations. D Damen and D Hogg. Computer Vision and Pattern Recognition (CVPR), Miami, Florida, June 2009.
      <a href="CVPR09.pdf">PDF</a> |
      <a href="CVPR09_poster.jpg">Poster</a>
    </p>
    
    <h3><a href="./BaggageDetection/baggage.htm">Detecting Carried Objects from Walking Pedestrians</a></h3>
    <img src="bags2.jpg" />
    
    <p><a href="https://www.youtube.com/watch?v=ZFPhr7mx4Mw">Video</a> |
      <a href="./BaggageDetection/LeedsBaggageDetector.zip">Code</a>
    </p>

    <p>Detecting Carried Objects from Sequences of Walking Pedestrians. D Damen and D Hogg. Pattern Analysis and Machine Intelligence (PAMI), 2012.
      <a href="http://dx.doi.org/10.1109/TPAMI.2011.205">PDF</a>.</p>

    <p>Detecting Carried Objects in Short Video Sequences. D Damen and D Hogg. European Conference on Computer Vision (ECCV), Marseille, France, Oct 2008
      <a href="DamenHoggECCV2008.pdf">PDF</a> |
      <a href="./BaggageDetection/ECCV08P.gif">Poster</a>
    </p>

   <h2>Research Group Members</h2>
    <ul>
      <li><a href="https://mwray.github.io">Michael Wray</a>, Postdoc, previously PhD student 2015-2019</li>
      <li><a href="http://willprice.org">Will Price</a>, PhD student 2017 - </li>
      <li><a href="https://ekazakos.github.io">Evangelos Kazakos</a>, PhD student 2017 - </li>
      <li><a href="https://jonmun.github.io">Jonathan Munro</a>, PhD student 2017 - </li>
      <li><a href="https://tobyperrett.github.io">Toby Perrett</a>, postdoc (SPHERE project), 2018-</li>
      <li><a href="https://majian8.github.io/jianma/">Jian Ma</a>, PhD student 2019 - </li>
      <li><a href="https://dwhettam.github.io">Daniel Whettam</a>, PhD student 2020 - (CDT in Interactive AI) </li>
      <li><a href="https://adrianofragomeni.github.io">Adriano Fragomeni</a>, PhD student 2020-</li>
      <li><a href="https://denabazazian.github.io">Dena Bazazian</a>, postdoc (LV project), 2021-</li>
      <li><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a>, PhD student 2021-</li>
      <li><a href="https://alexandrosstergiou.github.io">Alexanros Stergiou</a>, Postdoctoral Researcher 2021-</li>
      <li><a href="https://binzhubz.github.io">Bin Zhu</a>, Postdoctorall Researcher 2021-</li>
      <li><a hreff="https://jacobchalk.github.io/">Jacob Chalk</a>, PhD student 2021-</li>
      <li>Ahmad Dar Khalil, PhD student 2021-</li>
      <li>Zhifan Zhu, PhD student 2021-</li>
     <!-- <li><a href="https://xiaogangjia.github.io">Xiaogang Jia, PhD student 2021-</a></li>-->
      </ul>
    
    <h2>Previous Students, and Postdocs</h2>
    <ul>
        <li><a href="http://www.bristol.ac.uk/engineering/people/alessandro-masullo/index.html">Alessandro Masullo</a>, postdoc (SPHERE project), 2017-2021 - currently Lecturer at University of Bristol</li>
        <li><a href="https://youngkyoonjang.bitbucket.io">Youngkyoon Jang</a>, postdoc (GLANCE project), 2018-2021 - currentlly postdoc at Imperial College</li>
        <li><a href="https://hazeldoughty.github.io">Hazel Doughty</a>, PhD student 2016 - 2020 - currently postdoc at University of Amsterdam</li>
        <li><a href="https://vilab.blogs.ilrt.org/?cmt-management-team=farnoosh-heidarivincheh">Farnoosh Heidarivincheh</a>, PhD student 2015-2020 - currently postdoc at University of Bristol</li>
        <li><a href="http://www.davidemoltisanti.com/research">Davide Moltisanti</a>, PhD student 2015-2019 - currently postdoc at NTU Singapore</li>
        <li><a href="https://www.farscope.bris.ac.uk/miguel-lagunes-fortiz">Miguel Fortiz</a>, PhD student 2016 - 2019 (Co-supervisor: Walterio Mayol-Cuevas)</li>
        <li><a href="http://www.bris.ac.uk/engineering/people/victor-ponce-lopez/index.html">Victor Ponce Lopez</a>, postdoc (SPHERE project), 2017-2018</li>
        <li><a href="http://www.irc-sphere.ac.uk/uob-vahid">Vahid Soleimani</a>, PhD student 2014-2018 (Co-supervisor: Majid Mirmehdi)</li>
       <li><a href="http://www.irc-sphere.ac.uk/uob-yangdi">Yangdi Xu</a>, PhD student 2013-2018</li>
       <li>Toby Perrett, postdoc (LOCATE project), 2017-2018</li>
        <li><a href="https://scholar.google.co.uk/citations?user=SfOhCKIAAAAJ&hl=en">Teesid Leelasawassuk</a>, PhD student 2011-2016</li>
        <li><a href="https://scholar.google.es/citations?user=7FrKNIIAAAAJ&hl=en">Massimo Camplani</a>, postdoc (SPHERE project), 2013-2017</li>
        <li><a href="http://www.bristol.ac.uk/engineering/people/sion-l-hannuna/index.html">Sion Hannuna</a>, postdoc (SPHERE project), 2013-2017</li>
        <li><a href="http://people.uwe.ac.uk/Pages/person.aspx?accountname=campus%5Cl3-tao">Lili Tao</a>, postdoc (SPHERE project), 2013-2017</li>
        <li><a href="http://www.swansea.ac.uk/staff/science/computer-science/paiementa/">Adeline Paiment</a>, postdoc (SPHERE project), 2013-2016</li>
   </ul>
    
    <script>
      $(function() {
	  var olderNews = $("#olderNews");
	  olderNews.hide();
	  $("#showOlderNews").click(function() {
	      olderNews.show(1000);
	  });
      });
    </script>
  </body>
</html>
